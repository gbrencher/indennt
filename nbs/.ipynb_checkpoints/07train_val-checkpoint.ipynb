{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fc1947",
   "metadata": {},
   "source": [
    "# InSAR denoiser training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6286b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import torch\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import random\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879cfea",
   "metadata": {},
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b238030-bcca-46ec-abfd-fda11c63fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/home/jovyan/InSAR_denoising_CNN'\n",
    "subset_types = ['signal', 'noise', 'dem', 'murp', 'era5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed974-b767-4a60-9ea7-a81325080ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude non tif files, e.g. metadata\n",
    "def list_tifs(my_fns):\n",
    "    my_list = []\n",
    "    for i in my_fns:\n",
    "        if i[-4:] == '.tif':\n",
    "            my_list.append(i)\n",
    "    return my_list\n",
    "\n",
    "def subset_lists(main_dir, ds_type, subset_types):\n",
    "    path_d = {}\n",
    "    fn_list = []\n",
    "    for type in subset_type:\n",
    "        path_d[type] = f'{main_dir}/{ds_type}_subsets/{type}'\n",
    "        fn_list.append(list_tifs(os.listdir(path_d[type])))\n",
    "    return path_d, fn_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079031af-9af1-4e2f-ae4e-d65f724769eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d, train_list = subset_lists(main_dir, 'train', subset_types)\n",
    "val_d, val_list = subset_lists(main_dir, 'val', subset_types)\n",
    "test_d, test_list = subset_lists(main_dir, 'test', subset_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f43d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c39167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset \n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, path_d, transform=None, \n",
    "                 norm=True, center=False, blurnoise=False, flip=False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.signal_dir = path_d['signal']\n",
    "        self.noise_dir = path_d['noise']\n",
    "        self.dem_dir = path_d['dem']\n",
    "        self.era5_dir = path_d['era5']\n",
    "        self.murp_dir = path_d['murp']\n",
    "        self.norm = norm\n",
    "        self.center = center\n",
    "        self.blurnoise = blurnoise\n",
    "        self.flip = flip\n",
    "        \n",
    "    #dataset length\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "    \n",
    "    #load images\n",
    "    def __getitem__(self,idx):\n",
    "        signal_path = self.signal_dir+self.file_list[idx]\n",
    "        noise_path = self.noise_dir+self.file_list[idx]\n",
    "        dem_path = self.dem_dir+self.file_list[idx]\n",
    "        era5_path = self.era5_dir+self.file_list[idx]\n",
    "        murp_path = self.murp_dir+self.file_list[idx]\n",
    "        \n",
    "        signal = self.transform(Image.open(signal_path))\n",
    "        noise = self.transform(Image.open(noise_path))\n",
    "        dem = self.transform(Image.open(dem_path))\n",
    "        era5 = self.transform(Image.open(era5_path))\n",
    "        murp = self.transform(Image.open(murp_path))\n",
    "        \n",
    "        # Blur noise\n",
    "        if self.blurnoise == True: # blur noise to mitigate signal from non atmospheric sources\n",
    "            gblur = transforms.GaussianBlur(kernel_size=(7, 7), sigma=5)\n",
    "            noise = gblur(noise)\n",
    "        \n",
    "        # Generate scaled training images\n",
    "        scalar = np.round(np.random.lognormal(-1.5, 1.5), 3) # FOR PLOTTING: 0.2, 0.7\n",
    "        signal = signal*scalar*-1 #multiply by -1 because mintpy has a reversed sign convention\n",
    "        igram = noise+signal\n",
    "\n",
    "        era5_corr = igram-era5\n",
    "        murp_corr = igram-murp\n",
    "        \n",
    "        # correct hp\n",
    "        hp_filter = transforms.GaussianBlur(kernel_size=(25, 25), sigma=15)\n",
    "        train_filtered = hp_filter(train)\n",
    "        hp_corr = train - train_filtered\n",
    "        \n",
    "        # normalization between -1 and 1 as in Zhao et al. https://doi.org/10.1016/j.isprsjprs.2021.08.009\n",
    "        if self.norm == True:\n",
    "            igram = 2*(((igram-igram.min())/(igram.max()-igram.min())))-1\n",
    "            signal = 2*(((signal-igram.min())/(igram.max()-igram.min())))-1\n",
    "            dem = 2*(((dem-dem.min())/(dem.max()-dem.min())))-1\n",
    "\n",
    "            norm_dict = {}\n",
    "            norm_dict['norm_min'] = igram.min()\n",
    "            norm_dict['norm_max'] = igram.max()\n",
    "        \n",
    "        if self.center == True: # center target images on 0 \n",
    "            center_median = signal.median()\n",
    "            norm_dict['center'] = center_median\n",
    "            \n",
    "            igram = igram-center_median\n",
    "            signal = signal-center_median\n",
    "        \n",
    "        if self.flip==True:\n",
    "            flip_dim = []\n",
    "            if random.random() < 0.25:\n",
    "                flip_dim = [0]\n",
    "            elif random.random() > 0.25 and random.random() < 0.5:\n",
    "                flip_dim = [1]\n",
    "            elif random.random() > 0.5 and random.random() < 0.75:\n",
    "                flip_dim = [0, 1]\n",
    "            \n",
    "            igram = torch.flip(igram, flip_dim)\n",
    "            signal = torch.flip(signal, flip_dim)\n",
    "            era5_corr = torch.flip(era5_corr, flip_dim)\n",
    "            murp_corr = torch.flip(murp_corr, flip_dim)\n",
    "            hp_corr = torch.flip(hp_corr, flip_dim)\n",
    "            dem = torch.flip(dem, flip_dim)\n",
    "            \n",
    "         \n",
    "        return igram, signal, dem, era5_corr, murp_corr, hp_corr, norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e875e-3e03-403e-844e-eec379a1d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_norm(array, norm_dict, center=False):\n",
    "    if center==True:\n",
    "        array = array + norm_dict['center']\n",
    "    array = ((array+1)*((norm_dict['norm_max']-norm_dict['norm_min'])/2))+norm_dict['norm_min']\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48154e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data = dataset(train_list, train_d, transform=my_transforms, blurnoise=True, flip=True)\n",
    "val_data = dataset(val_list, val_d, transform=my_transforms, blurnoise=True)\n",
    "test_data = dataset(test_list, test_d, transform=my_transforms, blurnoise=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=1, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9287c",
   "metadata": {},
   "source": [
    "## Examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# igram, components, and dem\n",
    "\n",
    "num_images = 5\n",
    "\n",
    "for i, (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "            f, ax = plt.subplots(1, 4, figsize=(20,7))\n",
    "            ax[0].imshow(sample.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1) \n",
    "            ax[0].set_title('training')\n",
    "            ax[1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "            ax[1].set_title('target signal')\n",
    "            ax[2].imshow((sample.squeeze()-signal_target.squeeze()), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "            ax[2].set_title('target noise')\n",
    "            ax[3].imshow(dem.squeeze(), cmap='viridis', vmin=-1, vmax=1)\n",
    "            ax[3].set_title('DEM')\n",
    "            f.tight_layout()\n",
    "            \n",
    "            #f, ax = plt.subplots(1, 5, figsize=(15,3))\n",
    "            #ax[0].hist(sum(sum(sum(sample.tolist(), []), []), []), bins=40)\n",
    "            #ax[1].hist(sum(sum(sum(signal_target.tolist(), []), []), []), bins=40)\n",
    "            #ax[2].hist(sum(sum(sum((sample-signal_target).tolist(), []), []), []), bins=40)\n",
    "            #ax[3].hist(sum(sum(sum(dem.tolist(), []), []), []), bins=40)\n",
    "            #f.tight_layout()\n",
    "            plt.savefig(f'input{i}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a160dd-abd6-4cd7-b58c-2b209a0bcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# igram, target signal, corrected igrams\n",
    "\n",
    "num_images = 5\n",
    "\n",
    "for i, (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "        sample = undo_norm(sample.squeeze(), norm_dict)\n",
    "        signal_target = undo_norm(signal_target.squeeze(), norm_dict)\n",
    "            f, ax = plt.subplots(1, 4, figsize=(20,7))\n",
    "            ax[0].imshow(sample, cmap='RdBu_r', vmin=-5, vmax=5) \n",
    "            ax[0].set_title('training')\n",
    "            ax[1].imshow(signal_target, cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[1].set_title('target signal')\n",
    "            ax[2].imshow(era5_corr.squeeze()), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[2].set_title('ERA5 corrected')\n",
    "            ax[3].imshow(murp_corr.squeeze(), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[3].set_title('MuRP corrected')\n",
    "            ax[3].imshow(hp_corr.squeeze(), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[3].set_title('HP corrected')\n",
    "            f.tight_layout()\n",
    "            \n",
    "            #plt.savefig(f'input_correctons{i}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21150611",
   "metadata": {},
   "source": [
    "## Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "def check_valid_activation(choice):\n",
    "    if choice not in ['relu', 'lrelu', 'prelu']:\n",
    "        raise ValueError(f\"'{choice}' is not a valid activation function. Choose among ['relu', 'lrelu', 'prelu'].\\n\")\n",
    "\n",
    "\n",
    "def upconv(in_channels, out_channels, mode='transpose'):\n",
    "    # stride=2 implies upsampling by a factor of 2\n",
    "    get_up_mode = nn.ModuleDict([\n",
    "        ['bilinear', nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2), conv1x1(in_channels, out_channels))],\n",
    "        ['transpose', nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)]\n",
    "    ])\n",
    "\n",
    "    return get_up_mode[mode]\n",
    "\n",
    "\n",
    "def get_activation(choice):\n",
    "    activation_functions = nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['lrelu', nn.LeakyReLU(inplace=True)],\n",
    "        ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "    return activation_functions[choice]\n",
    "\n",
    "\n",
    "def conv_block(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Partial encoder block consisting of a 3×3 convolutional layer with stride 1, followed by batch normalization\n",
    "    (optional) and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "def conv_up_block(in_channels, out_channels, activation='relu', do_BN=True, up_mode='transpose', *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Decoder block consisting of an up-convolutional layer, followed by a 3×3 convolutional layer with stride 1,\n",
    "    batch normalization (optional), and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "\n",
    "\n",
    "def bottleneck(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Bottleneck block.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipConnection, self).__init__()\n",
    "\n",
    "    def forward(self, x_skip, x_up):\n",
    "        return x_skip + x_up\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_input_channels=2, start_kernel=64, max_filter_depth=512, depth=5,\n",
    "                 act_fn_encoder='relu', act_fn_decoder='relu', act_fn_bottleneck='relu', up_mode='transpose',\n",
    "                 do_BN=True, bias_conv_layer=False, outer_skip=True, outer_skip_BN=False):\n",
    "        \"\"\"\n",
    "        UNet network architecture.\n",
    "        :param n_input_channels:    int, number of input channels\n",
    "        :param start_kernel:        int, number of filters of the first convolutional layer in the encoder\n",
    "        :param max_filter_depth:    int, maximum filter depth\n",
    "        :param depth:               int, number of downsampling and upsampling layers (i.e., number of blocks in the\n",
    "                                    encoder and decoder)\n",
    "        :param act_fn_encoder:      str, activation function used in the encoder\n",
    "        :param act_fn_decoder:      str, activation function used in the decoder\n",
    "        :param act_fn_bottleneck:   str, activation function used in the bottleneck\n",
    "        :param up_mode:             str, upsampling mode\n",
    "        :param do_BN:               boolean, True to perform batch normalization after every convolutional layer,\n",
    "                                    False otherwise\n",
    "        :param bias_conv_layer:     boolean, True to activate the learnable bias of the convolutional layers,\n",
    "                                    False otherwise\n",
    "        :param outer_skip:          boolean, True to activate the long residual skip connection that adds the\n",
    "                                    initial DSM to the output of the last decoder layer, False otherwise\n",
    "        :param outer_skip_BN:       boolean, True to add batch normalization to the long residual skip connection,\n",
    "                                    False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        check_valid_activation(act_fn_encoder)\n",
    "        check_valid_activation(act_fn_decoder)\n",
    "        check_valid_activation(act_fn_bottleneck)\n",
    "\n",
    "        if up_mode not in ['transpose', 'bilinear']:\n",
    "            raise ValueError(f\"'{up_mode}' is not a valid mode for upsampling. Choose among ['transpose', 'bilinear'] \"\n",
    "                             \"to specify 'up_mode'.\\n\")\n",
    "\n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.start_kernel = start_kernel\n",
    "        self.depth = depth\n",
    "        self.act_fn_encoder = act_fn_encoder\n",
    "        self.act_fn_decoder = act_fn_decoder\n",
    "        self.act_fn_bottleneck = act_fn_bottleneck\n",
    "        self.up_mode = up_mode\n",
    "        self.max_filter_depth = max_filter_depth\n",
    "        self.do_BN = do_BN\n",
    "        self.bias_conv_layer = bias_conv_layer\n",
    "        self.do_outer_skip = outer_skip\n",
    "        self.do_outer_skip_BN = outer_skip_BN\n",
    "        self.filter_depths = [self.start_kernel * (2 ** i) for i in range(self.depth)]\n",
    "\n",
    "        # Restrict the maximum filter depth to a predefined value\n",
    "        self.filter_depths = [self.max_filter_depth if i > self.max_filter_depth else i for i in self.filter_depths]\n",
    "\n",
    "        # Set up the encoder\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.encoder.append(nn.Sequential(\n",
    "            conv_block(self.n_input_channels, self.start_kernel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths, self.filter_depths[1:]):\n",
    "            self.encoder.append(nn.Sequential(\n",
    "                conv_block(in_channel, out_channel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        # Set up the bottleneck\n",
    "        self.bottleneck = bottleneck(self.filter_depths[-1], self.filter_depths[-1], activation=self.act_fn_bottleneck,\n",
    "                                     do_BN=self.do_BN)\n",
    "\n",
    "        # Set up the decoder\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.filter_depths_up = list(reversed(self.filter_depths))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths_up[:-1], self.filter_depths_up[1:]):\n",
    "            self.decoder.append(conv_up_block(in_channel, out_channel, activation=self.act_fn_decoder,\n",
    "                                              up_mode=self.up_mode, do_BN=self.do_BN))\n",
    "        self.decoder.append(upconv(self.filter_depths_up[-1], self.filter_depths_up[-1], up_mode))\n",
    "\n",
    "        # Set up the final layer of the decoder\n",
    "        self.last_layer = conv3x3(self.start_kernel, 1, bias=self.bias_conv_layer)\n",
    "\n",
    "        # Skip connection\n",
    "        self.skipconnect = SkipConnection()\n",
    "\n",
    "        # Batch normalization added to the long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            self.layer_outer_skip = nn.ModuleList()\n",
    "            if self.do_outer_skip_BN:\n",
    "                self.layer_outer_skip.append(nn.BatchNorm2d(1))\n",
    "            self.layer_outer_skip.append(SkipConnection())\n",
    "\n",
    "    def forward(self, x, dem):\n",
    "        skip_connections = []\n",
    "        x = torch.cat((x, dem), dim=1)\n",
    "        out = x\n",
    "\n",
    "        # Encoder (save intermediate outputs for skip connections)\n",
    "        for index, layer in enumerate(self.encoder):\n",
    "            layer_conv = layer[:-1]  # all layers before the pooling layer (at depth index)\n",
    "            layer_pool = layer[-1]   # pooling layer (at depth index)\n",
    "\n",
    "            out_before_pool = layer_conv(out)\n",
    "            skip_connections.append(out_before_pool)\n",
    "            out = layer_pool(out_before_pool)\n",
    "\n",
    "        # Bottleneck\n",
    "        out = self.bottleneck(out)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        index_max = len(self.decoder) - 1\n",
    "        for index, layer in enumerate(self.decoder):\n",
    "            if index <= index_max - 1:\n",
    "                layer_upconv = layer[0]  # upconv layer\n",
    "                layer_conv = layer[1::]  # all other layers (conv, batchnorm, activation)\n",
    "\n",
    "                out_temp = layer_upconv(out)\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "                out = layer_conv(out)\n",
    "            else:\n",
    "                out_temp = layer(out)   # upconv of last layer\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "\n",
    "        # Last layer of the decoder\n",
    "        out = self.last_layer(out)\n",
    "\n",
    "        # Add long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            if self.layer_outer_skip.__len__() == 2:\n",
    "                # pipe input through a batch normalization layer before adding it to the output of the last\n",
    "                # decoder layer\n",
    "                bn = self.layer_outer_skip[0]\n",
    "                x_0 = x[:, 0, :, :]       # use channel 0 only\n",
    "                x_0 = x_0.unsqueeze(1)\n",
    "                x = bn(x_0)\n",
    "\n",
    "            # add (batchnorm) input to the output of the last decoder layer\n",
    "            add = self.layer_outer_skip[-1]\n",
    "            x_0 = x[:, 0, :, :]\n",
    "            x_0 = x_0.unsqueeze(1)\n",
    "\n",
    "            out = add(x_0, out)  # use channel 0 only\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df2ba7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc04b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load previous model\n",
    "# model = UNet()\n",
    "# model.load_state_dict(torch.load('noisemodelv4.4_150epochs'))\n",
    "# model.to('cuda')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9060e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Define optimizer\n",
    "model = UNet()\n",
    "model.to('cuda') # run on gpu\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.02) # set learning rate\n",
    "loss_fn   = nn.L1Loss() # MAE loss function, doesn't penalize outliers as much as MSE\n",
    "epochs = 150\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nstarting epoch {epoch}')\n",
    "    epoch_loss=[]\n",
    "    val_temp_loss = []\n",
    "    \n",
    "    #if epoch == 10:\n",
    "        #optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001) # reduce loss as given epoch\n",
    "    \n",
    "    #loop through training data \n",
    "    for (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = torch.clamp(model(sample.to('cuda'), dem.to('cuda')), -1, 1) # Generate noise predictions\n",
    "    \n",
    "        # calculate predicted signals \n",
    "        signal_pred = sample.to('cuda')-out.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(signal_pred.to('cuda'), signal_target.to('cuda')) # calculate loss \n",
    "        epoch_loss.append(loss.item()) # add batch loss to epoch loss list\n",
    "        \n",
    "        loss.backward() #Propagate the gradients in backward pass\n",
    "        optimizer.step() \n",
    "\n",
    "    train_loss.append(np.mean(epoch_loss))\n",
    "    print(f'training loss: {np.mean(epoch_loss)}')\n",
    "    \n",
    "    # run model on validation data \n",
    "    for (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in val_loader:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = torch.clamp(model(sample.to('cuda'), dem.to('cuda')), -1, 1) #Generate predictions using the model\n",
    "            \n",
    "            signal_pred = sample.to('cuda')-out.to('cuda')\n",
    "           \n",
    "            loss = loss_fn(signal_pred.to('cuda'), signal_target.to('cuda')) #Loss/error\n",
    "            val_temp_loss.append(loss.item())\n",
    "    \n",
    "    val_loss.append(np.mean(val_temp_loss))\n",
    "    print(f'validation loss: {np.mean(val_temp_loss)}')\n",
    "    \n",
    "    if (epoch+1)%25 == 0: \n",
    "        # save model\n",
    "        torch.save(model.state_dict(), f'noisemodel_temp4paper_noblur_{epoch+1}epochs')\n",
    "        \n",
    "    with open('val_loss_noblur.pkl', 'wb') as f:\n",
    "        pickle.dump(val_loss, f)\n",
    "        \n",
    "    with open('train_loss_noblur.pkl', 'wb') as f:\n",
    "        pickle.dump(train_loss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeefdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss over all epochs\n",
    "f, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(train_loss, label='training')\n",
    "ax.plot(val_loss, label='validation')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('L1 loss')\n",
    "ax.set_title('Loss')\n",
    "ax.legend()\n",
    "#plt.savefig('loss2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ef8a2-9b31-44cc-8da2-5fe8fa6b9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw inputs and outputs\n",
    "for i, (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "        with torch.no_grad():\n",
    "            noise = model(sample.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "            signal = torch.clamp(sample.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "            f, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "            ax[0][0].imshow(sample.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1) \n",
    "            ax[0][0].set_title('original interferogram')\n",
    "            ax[0][1].imshow((sample.squeeze()-signal_target.squeeze()), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "            ax[0][1].set_title('true noise')\n",
    "            ax[0][2].imshow(noise.squeeze().to('cpu'), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "            ax[0][2].set_title('predicted noise')\n",
    "            ax[1][0].imshow(dem.squeeze().to('cpu'), cmap='viridis', vmin=-1, vmax=1)\n",
    "            ax[1][0].set_title('DEM')\n",
    "            ax[1][1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "            ax[1][1].set_title('true signal')\n",
    "            ax[1][2].imshow(signal.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "            ax[1][2].set_title('predicted signal')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(f'pred_raw{i}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ce385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-normalized inputs and outputs\n",
    "for i, (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "        with torch.no_grad():\n",
    "            noise = model(sample.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "            signal = torch.clamp(sample.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "            \n",
    "            sample = undo_norm(sample.squeeze(), norm_dict)\n",
    "            signal_target = undo_norm(signal_target.squeeze(), norm_dict)\n",
    "            noise = undo_norm(noise.squeeze().to_cpu(), norm_dict)\n",
    "            signal = undo_norm(signal.squeeze(), norm_dict)\n",
    "            \n",
    "            f, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "            ax[0][0].imshow(sample, cmap='RdBu_r', vmin=-5, vmax=5) \n",
    "            ax[0][0].set_title('original interferogram')\n",
    "            ax[0][1].imshow((sample-signal_target), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[0][1].set_title('true noise')\n",
    "            ax[0][2].imshow(noise, cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[0][2].set_title('predicted noise')\n",
    "            ax[1][0].imshow(dem.squeeze().to('cpu'), cmap='viridis', vmin=-1, vmax=1)\n",
    "            ax[1][0].set_title('DEM')\n",
    "            ax[1][1].imshow(signal_target, cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[1][1].set_title('true signal')\n",
    "            ax[1][2].imshow(signal, cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "            ax[1][2].set_title('predicted signal')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(f'pred_raw{i}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705142f",
   "metadata": {},
   "source": [
    "## Evaluate results with training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4bf65-0f37-4987-b08b-703b9420a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloaders for evaluation\n",
    "val_data_ssim = dataset(val_list, val_d, transform=my_transforms, blurnoise=True)\n",
    "train_data_ssim = dataset(train_list, train_d, transform=my_transforms, blurnoise=True)\n",
    "val_loader_ssim = torch.utils.data.DataLoader(dataset = val_data_ssim, batch_size=1, shuffle=True)\n",
    "train_loader_ssim = torch.utils.data.DataLoader(dataset = train_data_ssim, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091c20f-d5c0-406d-ab8a-e5e1bbd0c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_lists(model, data_loader):\n",
    "    # initialize lists \n",
    "    ssim_list_uncorrected = []\n",
    "    ssim_list_model = []\n",
    "    ssim_list_era5 = []\n",
    "    ssim_list_murp = []\n",
    "    ssim_list_hp = []\n",
    "    \n",
    "    for i, (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in enumerate(data_loader):\n",
    "        # uncorrected SSIM\n",
    "        # denormalize\n",
    "        sample_raw = undo_norm(sample.squeeze().detach(), norm_dict)\n",
    "        signal_target_raw = undo_norm(signal_target.squeeze().detach(), norm_dict)\n",
    "        # calc ssim\n",
    "        ssim_value_uncorrected = ssim(sample_raw.numpy(), signal_target_raw.numpy(), gaussian_weights=True)\n",
    "        ssim_list_uncorrected.append(ssim_value_uncorrected)\n",
    "    \n",
    "        # model corrected SSIM\n",
    "        # model preds\n",
    "        noise = model(sample.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "        signal = torch.clamp(sample.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "        # denormalize\n",
    "        signal = undo_norm(signal.squeeze().detach(), norm_dict)\n",
    "        # calc ssim\n",
    "        ssim_value_model = ssim(signal.numpy(), signal_target_raw.numpy(), gaussian_weights=True)\n",
    "        ssim_list_model.append(ssim_value_model)\n",
    "    \n",
    "        # era5 corrected SSIM\n",
    "        ssim_value_era5 = ssim(era5_corr.squeeze().numpy(), signal_target_raw.numpy(),gaussian_weights=True)\n",
    "        ssim_list_era5.append(ssim_value_era5)\n",
    "\n",
    "        # murp corrected SSIM\n",
    "        ssim_value_murp = ssim(murp_corr.squeeze().numpy(), signal_target_raw.numpy(),gaussian_weights=True)\n",
    "        ssim_list_murp.append(ssim_value_murp)\n",
    "    \n",
    "        # hp filter corrected SSIM\n",
    "        ssim_value_hp = ssim(hp_corr.squeeze().numpy(), signal_target_raw.squeeze().numpy(), gaussian_weights=True)\n",
    "        ssim_list_hp.append(ssim_value_hp)\n",
    "    \n",
    "    print('median ssim before correction:', np.median(ssim_list_uncorrected),\n",
    "          '\\nmedian ssim model correction:', np.median(ssim_list_model), \n",
    "          '\\nmedian ssim era5 correction:', np.median(ssim_list_era5),\n",
    "          '\\nmedian ssim murp correction:', np.median(ssim_list_murp),\n",
    "          '\\nmedian ssim high pass filter correction:', np.median(ssim_list_hp))\n",
    "    \n",
    "    return ssim_list_uncorrected, ssim_list_model, ssim_list_era5, ssim_list_murp, ssim_list_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f103dd-b7c7-42b6-9f6e-3e9e3641220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('val data ssim')\n",
    "val_ssim_list_uncorrected, val_ssim_list_model, val_ssim_list_era5, val_ssim_list_murp, val_ssim_list_hp = ssim_lists(model, val_loader_ssim)\n",
    "print('training data ssim')\n",
    "train_ssim_list_uncorrected, train_ssim_list_model, train_ssim_list_era5, val_ssim_list_murp, train_ssim_list_hp = ssim_lists(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf8616-f708-4468-a87f-b1be211f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SNR\n",
    "def rms(tensor):\n",
    "    rms = np.sqrt(np.mean(tensor.squeeze().numpy()**2))\n",
    "    return rms\n",
    "\n",
    "def snr(model, data_loader):\n",
    "    snr_list = []\n",
    "\n",
    "    for i, (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in enumerate(data_loader):\n",
    "        sample_raw = undo_norm(sample.squeeze().detach(), norm_dict)\n",
    "        signal_target_raw = undo_norm(signal_target.squeeze().detach(), norm_dict)\n",
    "        snr_list.append(rms(signal_target_raw)/rms(sample-signal_target_raw))\n",
    "\n",
    "    print('median snr of images:', np.median(snr_list), 'stdev of SNR of images:', np.std(snr_list))\n",
    "    \n",
    "    return snr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785ce4f-4b81-46c5-9449-1a5016d4bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_snr_list = snr(model, val_loader_ssim)\n",
    "train_snr_list = snr(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9ab51-2289-47a6-bebd-42b4048ad98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(tensor):\n",
    "    rms = np.sqrt(np.mean(tensor.squeeze().numpy()**2))\n",
    "    return rms\n",
    "\n",
    "def snr_single(signal_target, sample):\n",
    "    snr_val = (rms(signal_target)/rms(sample-signal_target))\n",
    "    return snr_val\n",
    "\n",
    "def ssim_single(signal_target, pred):\n",
    "    ssim_val = ssim(pred.squeeze().detach().numpy(), \n",
    "                    signal_target.squeeze().detach().numpy(), gaussian_weights=True)\n",
    "    return ssim_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf500a-6fd3-41d4-88f0-dd107c99ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example corrections for paper fig\n",
    "plt.style.use('default')\n",
    "\n",
    "num_images = 1\n",
    "\n",
    "for i, (sample, signal_target, dem, era5_corr, murp_corr, hp_corr, norm_dict) in enumerate(train_loader_ssim):\n",
    "    if i < num_images:\n",
    "        noise_pred = model(sample.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "        signal_pred = torch.clamp(sample.to('cpu') - noise_pred.to('cpu'), -1, 1)\n",
    "\n",
    "        sample = undo_norm(sample.squeeze().detach(), norm_dict)\n",
    "        signal_target = undo_norm(signal_target.squeeze().detach(), norm_dict)\n",
    "        noise_pred = undo_norm(noise_pred.squeeze().detach(), norm_dict)\n",
    "        signal_pred = undo_norm(signal_pred.squeeze().detach(), norm_dict)\n",
    "        \n",
    "        # Interferogram SNR\n",
    "        print(f'interferogram SNR: {snr_single(signal_target, sample)}')\n",
    "        # Uncorrected ssim\n",
    "        print(f'uncorrected SSIM: {ssim_single(signal_target, sample)}')\n",
    "        # CNN corrected ssim\n",
    "        print(f'CNN corrected SSIM: {ssim_single(signal_target, signal_pred)}')\n",
    "        # ERA5 corrected ssim\n",
    "        print(f'ERA5 corrected SSIM: {ssim_single(signal_target, era5_corr)}')\n",
    "        # murp corrected ssim\n",
    "        print(f'MuRP corrected SSIM: {ssim_single(signal_target, murp_corr)}')\n",
    "        # HP corrected ssim\n",
    "        print(f'HP corrected SSIM: {ssim_single(signal_target, hp_corr)}')\n",
    "        \n",
    "        f, ax = plt.subplots(2, 5, figsize=(10,6))\n",
    "        # interferogram\n",
    "        ax[0, 0].imshow(sample.squeeze(), cmap='RdBu_r', vmin=-5, vmax=5) \n",
    "        ax[0, 0].set_title('training')\n",
    "        ax[0, 0].axis('off')\n",
    "        # dem\n",
    "        ax[0, 1].imshow(dem.squeeze(), cmap='Greys_r', vmin=-1, vmax=1) \n",
    "        ax[0, 1].set_title('DEM')\n",
    "        ax[0, 1].axis('off')\n",
    "        # target signal\n",
    "        ax[0, 2].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "        ax[0, 2].set_title('target signal')\n",
    "        ax[0, 2].axis('off')\n",
    "        #target noise\n",
    "        ax[0, 3].imshow((sample.squeeze()-signal_target.squeeze()), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "        ax[0, 3].set_title('target noise')\n",
    "        ax[0, 3].axis('off')\n",
    "        # CNN noise prediction\n",
    "        ax[1, 0].imshow(noise_pred.detach().squeeze().to('cpu'), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "        ax[1, 0].set_title('model noise prediction')\n",
    "        ax[1, 0].axis('off')\n",
    "        # CNN signal prediction\n",
    "        ax[1, 1].imshow(signal_pred.detach().squeeze(), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "        ax[1, 1].set_title('model corrected')\n",
    "        ax[1, 1].axis('off')\n",
    "        # ERA5 signal prediction\n",
    "        ax[1, 2].imshow(era5_corr.squeeze(), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "        ax[1, 2].set_title('ERA5 corrected')\n",
    "        ax[1, 2].axis('off')\n",
    "        # murp signal prediction\n",
    "        ax[1, 3].imshow(murp_corr.squeeze(), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "        ax[1, 3].set_title('MuRP corrected')\n",
    "        ax[1, 3].axis('off')\n",
    "        # HP signal prediction\n",
    "        ax[1, 4].imshow(hp_corr.squeeze(), cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "        ax[1, 4].set_title('hp filter corrected')\n",
    "        ax[1, 4].axis('off')\n",
    "        f.tight_layout()\n",
    "        \n",
    "        #plt.savefig('pred_example.png', dpi=300)\n",
    "    \n",
    "        if i+1 >= num_images:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137dc24d-18cd-4829-89db-8235b0acd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse plot\n",
    "\n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(1, 5, figsize=(10,4), sharex=True, sharey=True)\n",
    "\n",
    "# uncorrected \n",
    "sns.histplot(ax=ax[0], x=signal_target.squeeze().ravel(), y=sample.detach().squeeze().ravel(), \n",
    "             cmap='Greens', cbar=False, alpha=1, binwidth=0.02, vmax=200)\n",
    "ax[0].set_xlabel('target signal')\n",
    "ax[0].set_ylabel('uncorrected signal')\n",
    "ax[0].set_title('Before correction')\n",
    "ax[0].set_box_aspect(1)\n",
    "ax[0].set_xlim(-0.5, 1.25)\n",
    "ax[0].set_ylim(-0.5, 1.25)\n",
    "ax[0].set_yticks([-0.5, 0, 0.5, 1])\n",
    "\n",
    "# cnn predictions\n",
    "sns.histplot(ax=ax[1], x=signal_target.squeeze().ravel(), y=signal_pred.detach().squeeze().ravel(), \n",
    "             cmap='Greens', cbar=False, alpha=1, binwidth=0.02, vmax=200)\n",
    "ax[1].set_xlabel('target signal')\n",
    "ax[1].set_ylabel('predicted signal')\n",
    "ax[1].set_title('CNN predicton')\n",
    "ax[1].set_box_aspect(1)\n",
    "ax[1].set_xlim(-0.5, 1.25)\n",
    "ax[1].set_ylim(-0.5, 1.25)\n",
    "ax[1].set_yticks([-0.5, 0, 0.5, 1])\n",
    "\n",
    "# era5 predictions\n",
    "sns.histplot(ax=ax[2], x=signal_target.squeeze().ravel(), y=era5_corr.squeeze().ravel(), \n",
    "             cmap='Greens', cbar=False, alpha=1, binwidth=0.02, vmax=200)\n",
    "ax[2].set_xlabel('target signal')\n",
    "ax[2].set_title('ERA5 prediction')\n",
    "ax[2].set_box_aspect(1)\n",
    "\n",
    "# murp predictions\n",
    "sns.histplot(ax=ax[3], x=signal_target.squeeze().ravel(), y=murp_corr.squeeze().ravel(), \n",
    "             cmap='Greens', cbar=False, alpha=1, binwidth=0.02, vmax=200)\n",
    "ax[3].set_xlabel('target signal')\n",
    "ax[3].set_title('MuRP prediction')\n",
    "ax[3].set_box_aspect(1)\n",
    "\n",
    "sns.histplot(ax=ax[4], x=signal_target.squeeze().ravel(), y=hp_corr.squeeze().ravel(), \n",
    "             cmap='Greens', cbar=False, alpha=1, binwidth=0.02, vmax=200)\n",
    "ax[4].set_xlabel('target signal')\n",
    "ax[4].set_title('high-pass filter prediction')\n",
    "ax[4].set_box_aspect(1)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('pred_example_resid.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165639a-70b1-4efe-9d85-738df7c394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_for_plotting(snr_list, ssim_list_uncorrected, ssim_list_model, ssim_list_era5, ssim_list_hp):\n",
    "\n",
    "    roll_count = 200\n",
    "    q_low = 25\n",
    "    q_high = 75\n",
    "\n",
    "    ssim_dict = {'snr': snr_list,\n",
    "                 'ssim_uncorrected':ssim_list_uncorrected,\n",
    "                 'ssim_model':ssim_list_model,\n",
    "                 'ssim_era5':ssim_list_era5,\n",
    "                 'ssim_murp':ssim_list_murp,\n",
    "                 'ssim_hp':ssim_list_hp}\n",
    "    ssim_df = pd.DataFrame(ssim_dict)\n",
    "\n",
    "    # uncorrected ssim\n",
    "    ssim_df['ssim_uncorrected_median'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).median()\n",
    "    ssim_df[f'ssim_uncorrected_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_uncorrected_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # model corrected ssim\n",
    "    ssim_df['ssim_model_median'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).median()\n",
    "    ssim_df[f'ssim_model_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_model_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # era5 corrected ssim\n",
    "    ssim_df['ssim_era5_median'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).median()\n",
    "    ssim_df[f'ssim_era5_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_era5_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # murp corrected ssim\n",
    "    ssim_df['ssim_murp_median'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).median()\n",
    "    ssim_df[f'ssim_murp_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_murp_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # era5 corrected ssim\n",
    "    ssim_df['ssim_hp_median'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).median()\n",
    "    ssim_df[f'ssim_hp_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_hp_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "    \n",
    "    return ssim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6d18c-cbc9-4bb3-838d-1069f780603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ssim_df=df_for_plotting(val_snr_list, val_ssim_list_uncorrected, val_ssim_list_model, val_ssim_list_era5, val_ssim_list_murp, val_ssim_list_hp)\n",
    "train_ssim_df=df_for_plotting(train_snr_list, train_ssim_list_uncorrected, train_ssim_list_model, train_ssim_list_era5, train_ssim_list_murp, train_ssim_list_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b338a4-598d-4693-9a11-879291ece522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't plot exactly 0 snr due to log scale\n",
    "train_ssim_df_clean = train_ssim_df[train_ssim_df.snr != 0]\n",
    "val_ssim_df_clean = val_ssim_df[val_ssim_df.snr != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c04f4-566f-4bfe-bbdd-fd1610fe4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "# val uncorrected \n",
    "sns.histplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False, alpha=0)\n",
    "ax[0, 0].set_xscale('log')\n",
    "\n",
    "sns.histplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 0].set_xscale('log')\n",
    "ax[0, 0].set_ylabel('SSIM')\n",
    "ax[0, 0].set_title('uncorrected')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val model corrected\n",
    "sns.histplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 1].set_xscale('log')\n",
    "ax[0, 1].set_ylabel('SSIM')\n",
    "ax[0, 1].set_title('CNN')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val era5 corrected\n",
    "sns.histplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 2].set_xscale('log')\n",
    "ax[0, 2].set_ylabel('SSIM')\n",
    "ax[0, 2].set_title('ERA5')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val murp corrected\n",
    "sns.histplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 3].set_xscale('log')\n",
    "ax[0, 3].set_ylabel('SSIM')\n",
    "ax[0, 3].set_title('MuRP')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "# val hp corrected\n",
    "sns.histplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 3].set_xscale('log')\n",
    "ax[0, 3].set_xlabel('SNR')\n",
    "ax[0, 3].set_ylabel('SSIM')\n",
    "ax[0, 3].set_title('low-pass filter')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train uncorrected \n",
    "sns.histplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 0].set_xscale('log')\n",
    "ax[1, 0].set_ylabel('SSIM')\n",
    "ax[1, 0].set_xlabel('SNR')\n",
    "\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train model corrected\n",
    "sns.histplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 1].set_xscale('log')\n",
    "ax[1, 1].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train era5 corrected\n",
    "sns.histplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 2].set_xscale('log')\n",
    "ax[1, 2].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train murp corrected\n",
    "sns.histplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 2].set_xscale('log')\n",
    "ax[1, 2].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train hp corrected\n",
    "sns.histplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 3].set_xscale('log')\n",
    "ax[1, 3].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_median, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "#plt.savefig('SSIMv2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab2cb0-6400-435c-8f1a-ab3248e432be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to long format for more plotting\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "val_ssim_long = pd.melt(val_ssim_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "val_ssim_long['dataset'] = 'val'\n",
    "\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "train_ssim_long = pd.melt(train_ssim_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "train_ssim_long['dataset'] = 'train'\n",
    "\n",
    "all_ssim_long = pd.concat([train_ssim_long, val_ssim_long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b6e15-e001-4101-9aee-2a2f6d569f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplots \n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "sns.kdeplot(ax=ax[1], data=val_ssim_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "sns.kdeplot(ax=ax[0], data=train_ssim_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "ax[1].set_xlim((-0.25, 1.15))\n",
    "ax[0].set_xlim((-0.25, 1.15))\n",
    "ax[1].set_xlabel('SSIM')\n",
    "ax[0].set_xlabel('SSIM')\n",
    "ax[1].set_ylabel('kernel density')\n",
    "ax[0].set_ylabel('kernel density')\n",
    "f.tight_layout()\n",
    "plt.savefig('ssim_kde.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gda]",
   "language": "python",
   "name": "conda-env-gda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
