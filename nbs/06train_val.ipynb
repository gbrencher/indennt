{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fc1947",
   "metadata": {},
   "source": [
    "# InSAR denoiser training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6286b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import torch\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "import random\n",
    "from scipy import stats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879cfea",
   "metadata": {},
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b238030-bcca-46ec-abfd-fda11c63fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/mnt/working/brencher/sw/repos/indennt'\n",
    "subset_types = ['signal', 'noise', 'dem', 'murp', 'era5']\n",
    "\n",
    "# exclude non tif files, e.g. metadata\n",
    "def list_tifs(my_fns):\n",
    "    my_list = []\n",
    "    for i in my_fns:\n",
    "        if i[-4:] == '.tif':\n",
    "            my_list.append(i)\n",
    "    return my_list\n",
    "\n",
    "def subset_lists(main_dir, ds_type, subset_types):\n",
    "    path_d = {}\n",
    "    fn_list = []\n",
    "    for type in subset_types:\n",
    "        path_d[type] = f'{main_dir}/{ds_type}_subsets/{type}/'\n",
    "        fn_list.append(list_tifs(os.listdir(path_d[type])))\n",
    "    return path_d, fn_list[0]\n",
    "\n",
    "train_d, train_list = subset_lists(main_dir, 'train', subset_types)\n",
    "val_d, val_list = subset_lists(main_dir, 'val', subset_types)\n",
    "#test_d, test_list = subset_lists(main_dir, 'test', subset_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f43d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c39167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset \n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, path_d, norm_list = [-41, 41, 0, 4374.6372], transform=None, \n",
    "                 norm=True, blurnoise=False, flip=False, invert=False, rotate=False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.signal_dir = path_d['signal']\n",
    "        self.noise_dir = path_d['noise']\n",
    "        self.dem_dir = path_d['dem']\n",
    "        self.era5_dir = path_d['era5']\n",
    "        self.murp_dir = path_d['murp']\n",
    "        self.norm = norm\n",
    "        self.blurnoise = blurnoise\n",
    "        self.flip = flip\n",
    "        self.invert = invert\n",
    "        self.rotate = rotate\n",
    "        self.igram_min = norm_list[0]\n",
    "        self.igram_max = norm_list[1]\n",
    "        self.dem_min = norm_list[2]\n",
    "        self.dem_max = norm_list[3]\n",
    "        \n",
    "    #dataset length\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "    \n",
    "    #load images\n",
    "    def __getitem__(self,idx):\n",
    "        signal_path = self.signal_dir+self.file_list[idx]\n",
    "        noise_path = self.noise_dir+self.file_list[idx]\n",
    "        dem_path = self.dem_dir+self.file_list[idx]\n",
    "        era5_path = self.era5_dir+self.file_list[idx]\n",
    "        murp_path = self.murp_dir+self.file_list[idx]\n",
    "        \n",
    "        signal = self.transform(Image.open(signal_path))\n",
    "        noise = self.transform(Image.open(noise_path))\n",
    "        dem = self.transform(Image.open(dem_path))\n",
    "        era5 = self.transform(Image.open(era5_path))\n",
    "        murp = self.transform(Image.open(murp_path))\n",
    "        \n",
    "        # Blur noise\n",
    "        if self.blurnoise == True: # blur noise to mitigate signal from non atmospheric sources\n",
    "            gblur = transforms.GaussianBlur(kernel_size=(17, 17), sigma=4)\n",
    "            noise = gblur(noise)\n",
    "        \n",
    "        # Generate scaled training images\n",
    "        scalar = np.round(np.random.lognormal(3, 0.7), 3) # FOR PLOTTING: 0.2, 0.7\n",
    "        signal = signal*scalar*-1 #multiply by -1 because mintpy has a reversed sign convention\n",
    "        igram = noise+signal\n",
    "\n",
    "        era5_corr = igram-era5\n",
    "        murp_corr = igram-murp\n",
    "        \n",
    "        # correct hp\n",
    "        hp_filter = transforms.GaussianBlur(kernel_size=(17, 17), sigma=4)\n",
    "        igram_filtered = hp_filter(igram)\n",
    "        hp_corr = igram - igram_filtered\n",
    "        \n",
    "        if self.invert==True:\n",
    "            if random.random() < 0.5:\n",
    "                igram = igram*-1\n",
    "                signal = signal*-1\n",
    "                noise = noise*-1\n",
    "                era5_corr = era5_corr *-1\n",
    "                murp_corr = murp_corr *-1\n",
    "                hp_corr = hp_corr *-1\n",
    "            \n",
    "        if self.norm == True:\n",
    "            igram = 2*(((igram-self.igram_min)/(self.igram_max-self.igram_min)))-1\n",
    "            signal = 2*(((signal-self.igram_min)/(self.igram_max-self.igram_min)))-1\n",
    "            noise = 2*(((noise-self.igram_min)/(self.igram_max-self.igram_min)))-1\n",
    "            dem = 2*(((dem-self.dem_min)/(self.dem_max-self.dem_min)))-1\n",
    "        \n",
    "        if self.flip==True:\n",
    "            flip_dim = []\n",
    "            if random.random() < 0.25:\n",
    "                flip_dim = [0]\n",
    "            elif random.random() > 0.25 and random.random() < 0.5:\n",
    "                flip_dim = [1]\n",
    "            elif random.random() > 0.5 and random.random() < 0.75:\n",
    "                flip_dim = [0, 1]\n",
    "            \n",
    "            igram = torch.flip(igram, flip_dim)\n",
    "            signal = torch.flip(signal, flip_dim)\n",
    "            noise = torch.flip(noise, flip_dim)\n",
    "            era5_corr = torch.flip(era5_corr, flip_dim)\n",
    "            murp_corr = torch.flip(murp_corr, flip_dim)\n",
    "            hp_corr = torch.flip(hp_corr, flip_dim)\n",
    "            dem = torch.flip(dem, flip_dim)\n",
    "        \n",
    "        if self.rotate==True:\n",
    "            angle = random.randint(0, 180)\n",
    "            igram = transforms.functional.rotate(igram, angle)\n",
    "            signal = transforms.functional.rotate(signal, angle)\n",
    "            noise = transforms.functional.rotate(noise, angle)\n",
    "            era5_corr = transforms.functional.rotate(era5_corr, angle)\n",
    "            murp_corr = transforms.functional.rotate(murp_corr, angle)\n",
    "            hp_corr = transforms.functional.rotate(hp_corr, angle)\n",
    "            dem = transforms.functional.rotate(dem, angle)\n",
    "            \n",
    "         \n",
    "        return igram, signal, noise, dem, era5_corr, murp_corr, hp_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049ef07-e52a-48dd-ae1f-8a268a3502e0",
   "metadata": {},
   "source": [
    "### Find dataset max and min for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14433430-133e-4506-aea6-69a5d14279c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset min and max for normalization\n",
    "# def get_max(data_list, data_d, transforms=my_transforms):\n",
    "#     max = 0 \n",
    "#     min = 0\n",
    "#     data = dataset(data_list, data_d, transform=transforms, blurnoise=True, norm=False)\n",
    "#     loader = torch.utils.data.DataLoader(dataset = data, batch_size=1, shuffle=False)\n",
    "#     for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(loader):\n",
    "#         if (i+1)%1000 == 0: \n",
    "#             print(f'loop {i+1}/{data.filelength}')\n",
    "#         if igram.max() > max:\n",
    "#             max = igram.max()\n",
    "\n",
    "#     return max\n",
    "\n",
    "# get_max(train_list, train_d)\n",
    "\n",
    "# # train igram max = 35.4796\n",
    "# # val igram max = 27.1600\n",
    "# # will set at 37 to be safe\n",
    "\n",
    "# # train dem max, min: 4374.6372, 0\n",
    "# # val dem max, min: 4353.3779, 0\n",
    "\n",
    "# # updating snr for run 8: \n",
    "# # train igram max: 36.0537\n",
    "# # will set at 37 to be safe\n",
    "\n",
    "# # updating snr for run 9 \n",
    "# # train igram max: 39.7634\n",
    "# # will set at 41 to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80e875e-3e03-403e-844e-eec379a1d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_norm(array, min=-41, max=41):\n",
    "    array = ((array+1)*((max-min)/2))+min\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48154e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data = dataset(train_list, train_d, transform=my_transforms, blurnoise=True, flip=True, invert=True, rotate=True)\n",
    "val_data = dataset(val_list, val_d, transform=my_transforms, blurnoise=True)\n",
    "#test_data = dataset(test_list, test_d, transform=my_transforms, blurnoise=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=1, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9287c",
   "metadata": {},
   "source": [
    "## Examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5ba1c-dddd-47bb-96e5-fc441c9b047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all original data \n",
    "num_images = 5\n",
    "\n",
    "for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "        igram = undo_norm(igram.squeeze())\n",
    "        signal_target = undo_norm(signal_target.squeeze())\n",
    "        noise_target = undo_norm(noise_target.squeeze())\n",
    "        \n",
    "        f, ax = plt.subplots(2, 5, figsize=(10,5), sharey=True, sharex=True)\n",
    "        ax[0][0].imshow(igram, cmap='RdBu_r', vmin=-10, vmax=10) \n",
    "        ax[0][0].set_title('training')\n",
    "        ax[0][1].imshow(signal_target, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        ax[0][1].set_title('target signal')\n",
    "        ax[0][2].imshow(era5_corr.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        ax[0][2].set_title('ERA5 corrected')\n",
    "        ax[0][3].imshow(murp_corr.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        ax[0][3].set_title('MuRP corrected')\n",
    "        ax[0][4].imshow(hp_corr.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        ax[0][4].set_title('HP corrected')\n",
    "        \n",
    "        ax[1][0].imshow(dem.squeeze(), cmap='viridis') \n",
    "        ax[1][0].set_title('DEM')\n",
    "        ax[1][1].imshow(noise_target, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1][1].set_title('target noise')\n",
    "        ax[1][2].imshow((igram-era5_corr.squeeze()), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1][2].set_title('ERA5 noise')\n",
    "        ax[1][3].imshow((igram-murp_corr.squeeze()), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1][3].set_title('MuRP noise')\n",
    "        ax[1][4].imshow((igram-hp_corr.squeeze()), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1][4].set_title('HP noise')\n",
    "        f.tight_layout()\n",
    "    else:\n",
    "        break\n",
    "            \n",
    "            #plt.savefig(f'input_correctons{i}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot normalized training inputs \n",
    "num_images = 5\n",
    "\n",
    "for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "            f, ax = plt.subplots(1, 4, figsize=(10,4), sharey=True)\n",
    "            ax[0].imshow(igram.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None) \n",
    "            ax[0].set_title('training')\n",
    "            ax[1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "            ax[1].set_title('target signal')\n",
    "            ax[2].imshow(noise_target.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "            ax[2].set_title('target noise')\n",
    "            ax[3].imshow(dem.squeeze(), cmap='viridis', vmin=-1, vmax=1, interpolation=None)\n",
    "            ax[3].set_title('DEM')\n",
    "            f.tight_layout()\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21150611",
   "metadata": {},
   "source": [
    "## Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd5508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "def check_valid_activation(choice):\n",
    "    if choice not in ['relu', 'lrelu', 'prelu']:\n",
    "        raise ValueError(f\"'{choice}' is not a valid activation function. Choose among ['relu', 'lrelu', 'prelu'].\\n\")\n",
    "\n",
    "\n",
    "def upconv(in_channels, out_channels, mode='transpose'):\n",
    "    # stride=2 implies upsampling by a factor of 2\n",
    "    get_up_mode = nn.ModuleDict([\n",
    "        ['bilinear', nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2), conv1x1(in_channels, out_channels))],\n",
    "        ['transpose', nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)]\n",
    "    ])\n",
    "\n",
    "    return get_up_mode[mode]\n",
    "\n",
    "\n",
    "def get_activation(choice):\n",
    "    activation_functions = nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['lrelu', nn.LeakyReLU(inplace=True)],\n",
    "        ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "    return activation_functions[choice]\n",
    "\n",
    "\n",
    "def conv_block(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Partial encoder block consisting of a 3×3 convolutional layer with stride 1, followed by batch normalization\n",
    "    (optional) and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "def conv_up_block(in_channels, out_channels, activation='relu', do_BN=True, up_mode='transpose', *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Decoder block consisting of an up-convolutional layer, followed by a 3×3 convolutional layer with stride 1,\n",
    "    batch normalization (optional), and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "\n",
    "\n",
    "def bottleneck(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Bottleneck block.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipConnection, self).__init__()\n",
    "\n",
    "    def forward(self, x_skip, x_up):\n",
    "        return x_skip + x_up\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_input_channels=2, start_kernel=64, max_filter_depth=512, depth=5,\n",
    "                 act_fn_encoder='relu', act_fn_decoder='relu', act_fn_bottleneck='relu', up_mode='transpose',\n",
    "                 do_BN=False, bias_conv_layer=True, outer_skip=True, outer_skip_BN=False):\n",
    "        \"\"\"\n",
    "        UNet network architecture.\n",
    "        :param n_input_channels:    int, number of input channels\n",
    "        :param start_kernel:        int, number of filters of the first convolutional layer in the encoder\n",
    "        :param max_filter_depth:    int, maximum filter depth\n",
    "        :param depth:               int, number of downsampling and upsampling layers (i.e., number of blocks in the\n",
    "                                    encoder and decoder)\n",
    "        :param act_fn_encoder:      str, activation function used in the encoder\n",
    "        :param act_fn_decoder:      str, activation function used in the decoder\n",
    "        :param act_fn_bottleneck:   str, activation function used in the bottleneck\n",
    "        :param up_mode:             str, upsampling mode\n",
    "        :param do_BN:               boolean, True to perform batch normalization after every convolutional layer,\n",
    "                                    False otherwise\n",
    "        :param bias_conv_layer:     boolean, True to activate the learnable bias of the convolutional layers,\n",
    "                                    False otherwise\n",
    "        :param outer_skip:          boolean, True to activate the long residual skip connection that adds the\n",
    "                                    initial DSM to the output of the last decoder layer, False otherwise\n",
    "        :param outer_skip_BN:       boolean, True to add batch normalization to the long residual skip connection,\n",
    "                                    False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        check_valid_activation(act_fn_encoder)\n",
    "        check_valid_activation(act_fn_decoder)\n",
    "        check_valid_activation(act_fn_bottleneck)\n",
    "\n",
    "        if up_mode not in ['transpose', 'bilinear']:\n",
    "            raise ValueError(f\"'{up_mode}' is not a valid mode for upsampling. Choose among ['transpose', 'bilinear'] \"\n",
    "                             \"to specify 'up_mode'.\\n\")\n",
    "\n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.start_kernel = start_kernel\n",
    "        self.depth = depth\n",
    "        self.act_fn_encoder = act_fn_encoder\n",
    "        self.act_fn_decoder = act_fn_decoder\n",
    "        self.act_fn_bottleneck = act_fn_bottleneck\n",
    "        self.up_mode = up_mode\n",
    "        self.max_filter_depth = max_filter_depth\n",
    "        self.do_BN = do_BN\n",
    "        self.bias_conv_layer = bias_conv_layer\n",
    "        self.do_outer_skip = outer_skip\n",
    "        self.do_outer_skip_BN = outer_skip_BN\n",
    "        self.filter_depths = [self.start_kernel * (2 ** i) for i in range(self.depth)]\n",
    "\n",
    "        # Restrict the maximum filter depth to a predefined value\n",
    "        self.filter_depths = [self.max_filter_depth if i > self.max_filter_depth else i for i in self.filter_depths]\n",
    "\n",
    "        # Set up the encoder\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.encoder.append(nn.Sequential(\n",
    "            conv_block(self.n_input_channels, self.start_kernel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths, self.filter_depths[1:]):\n",
    "            self.encoder.append(nn.Sequential(\n",
    "                conv_block(in_channel, out_channel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        # Set up the bottleneck\n",
    "        self.bottleneck = bottleneck(self.filter_depths[-1], self.filter_depths[-1], activation=self.act_fn_bottleneck,\n",
    "                                     do_BN=self.do_BN)\n",
    "\n",
    "        # Set up the decoder\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.filter_depths_up = list(reversed(self.filter_depths))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths_up[:-1], self.filter_depths_up[1:]):\n",
    "            self.decoder.append(conv_up_block(in_channel, out_channel, activation=self.act_fn_decoder,\n",
    "                                              up_mode=self.up_mode, do_BN=self.do_BN))\n",
    "        self.decoder.append(upconv(self.filter_depths_up[-1], self.filter_depths_up[-1], up_mode))\n",
    "\n",
    "        # Set up the final layer of the decoder\n",
    "        self.last_layer = conv3x3(self.start_kernel, 1, bias=self.bias_conv_layer)\n",
    "\n",
    "        # Skip connection\n",
    "        self.skipconnect = SkipConnection()\n",
    "\n",
    "        # Batch normalization added to the long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            self.layer_outer_skip = nn.ModuleList()\n",
    "            if self.do_outer_skip_BN:\n",
    "                self.layer_outer_skip.append(nn.BatchNorm2d(1))\n",
    "            self.layer_outer_skip.append(SkipConnection())\n",
    "\n",
    "    def forward(self, x, dem):\n",
    "        skip_connections = []\n",
    "        x = torch.cat((x, dem), dim=1)\n",
    "        out = x\n",
    "\n",
    "        # Encoder (save intermediate outputs for skip connections)\n",
    "        for index, layer in enumerate(self.encoder):\n",
    "            layer_conv = layer[:-1]  # all layers before the pooling layer (at depth index)\n",
    "            layer_pool = layer[-1]   # pooling layer (at depth index)\n",
    "\n",
    "            out_before_pool = layer_conv(out)\n",
    "            skip_connections.append(out_before_pool)\n",
    "            out = layer_pool(out_before_pool)\n",
    "\n",
    "        # Bottleneck\n",
    "        out = self.bottleneck(out)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        index_max = len(self.decoder) - 1\n",
    "        for index, layer in enumerate(self.decoder):\n",
    "            if index <= index_max - 1:\n",
    "                layer_upconv = layer[0]  # upconv layer\n",
    "                layer_conv = layer[1::]  # all other layers (conv, batchnorm, activation)\n",
    "\n",
    "                out_temp = layer_upconv(out)\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "                out = layer_conv(out)\n",
    "            else:\n",
    "                out_temp = layer(out)   # upconv of last layer\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "\n",
    "        # Last layer of the decoder\n",
    "        out = self.last_layer(out)\n",
    "\n",
    "        # Add long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            if self.layer_outer_skip.__len__() == 2:\n",
    "                # pipe input through a batch normalization layer before adding it to the output of the last\n",
    "                # decoder layer\n",
    "                bn = self.layer_outer_skip[0]\n",
    "                x_0 = x[:, 0, :, :]       # use channel 0 only\n",
    "                x_0 = x_0.unsqueeze(1)\n",
    "                x = bn(x_0)\n",
    "\n",
    "            # add (batchnorm) input to the output of the last decoder layer\n",
    "            add = self.layer_outer_skip[-1]\n",
    "            x_0 = x[:, 0, :, :]\n",
    "            x_0 = x_0.unsqueeze(1)\n",
    "\n",
    "            out = add(x_0, out)  # use channel 0 only\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df2ba7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc04b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (last_layer): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (skipconnect): SkipConnection()\n",
       "  (layer_outer_skip): ModuleList(\n",
       "    (0): SkipConnection()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load previous model\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load('../models/noisemodel1.3_195epochs'))\n",
    "model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada9060e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting epoch 195\n",
      "training loss: 0.001761052154440257\n",
      "validation loss: 0.002277008163438739\n",
      "\n",
      "starting epoch 196\n",
      "training loss: 0.0017547646116229502\n",
      "validation loss: 0.0022438734159690933\n",
      "\n",
      "starting epoch 197\n",
      "training loss: 0.0017379365652567774\n",
      "validation loss: 0.0021843213365644503\n",
      "\n",
      "starting epoch 198\n",
      "training loss: 0.00176723632794633\n",
      "validation loss: 0.002190024466377741\n",
      "\n",
      "starting epoch 199\n",
      "training loss: 0.0017607443653198974\n",
      "validation loss: 0.0022621435890613603\n",
      "\n",
      "starting epoch 200\n",
      "training loss: 0.001748496183402069\n",
      "validation loss: 0.002149203629353717\n",
      "\n",
      "starting epoch 201\n",
      "training loss: 0.0017673588295910876\n",
      "validation loss: 0.0022104915772742403\n",
      "\n",
      "starting epoch 202\n",
      "training loss: 0.0017556662705243272\n",
      "validation loss: 0.002187492619329508\n",
      "\n",
      "starting epoch 203\n",
      "training loss: 0.0017514423756508453\n",
      "validation loss: 0.0021523663501971874\n",
      "\n",
      "starting epoch 204\n",
      "training loss: 0.001754537233263414\n",
      "validation loss: 0.0021569782115594557\n",
      "\n",
      "starting epoch 205\n",
      "training loss: 0.001743668600139428\n",
      "validation loss: 0.0022483063981661227\n",
      "\n",
      "starting epoch 206\n",
      "training loss: 0.001748832178505961\n",
      "validation loss: 0.002254731726037052\n",
      "\n",
      "starting epoch 207\n",
      "training loss: 0.0017434327067235732\n",
      "validation loss: 0.0022329098437263734\n",
      "\n",
      "starting epoch 208\n",
      "training loss: 0.0017533520338537227\n",
      "validation loss: 0.0022579020699836907\n",
      "\n",
      "starting epoch 209\n",
      "training loss: 0.001738913295552726\n",
      "validation loss: 0.0022732214605792065\n",
      "\n",
      "starting epoch 210\n",
      "training loss: 0.0017559953199303808\n",
      "validation loss: 0.0022889454923432054\n",
      "\n",
      "starting epoch 211\n",
      "training loss: 0.0017407946428483903\n",
      "validation loss: 0.0021802871796335788\n",
      "\n",
      "starting epoch 212\n",
      "training loss: 0.0017529595773187446\n",
      "validation loss: 0.002262363612431177\n",
      "\n",
      "starting epoch 213\n",
      "training loss: 0.0017362060107774394\n",
      "validation loss: 0.002218581276885278\n",
      "\n",
      "starting epoch 214\n",
      "training loss: 0.0017272404541437366\n",
      "validation loss: 0.002244579497790462\n",
      "\n",
      "starting epoch 215\n",
      "training loss: 0.0017272334367480497\n",
      "validation loss: 0.00215665192308139\n",
      "\n",
      "starting epoch 216\n",
      "training loss: 0.0017231461782380138\n",
      "validation loss: 0.0022546776605390974\n",
      "\n",
      "starting epoch 217\n",
      "training loss: 0.001740000386074857\n",
      "validation loss: 0.0021871672766143476\n",
      "\n",
      "starting epoch 218\n",
      "training loss: 0.0017301479345507313\n",
      "validation loss: 0.0023450393363204144\n",
      "\n",
      "starting epoch 219\n",
      "training loss: 0.001749542928520638\n",
      "validation loss: 0.0022932947913723413\n",
      "\n",
      "starting epoch 220\n",
      "training loss: 0.001718004013295785\n",
      "validation loss: 0.002202802882579155\n",
      "\n",
      "starting epoch 221\n",
      "training loss: 0.0017445498881082135\n",
      "validation loss: 0.002232470478616303\n",
      "\n",
      "starting epoch 222\n",
      "training loss: 0.0017448479647932038\n",
      "validation loss: 0.0022567866279659173\n",
      "\n",
      "starting epoch 223\n",
      "training loss: 0.00172606376304527\n",
      "validation loss: 0.002219969249770707\n",
      "\n",
      "starting epoch 224\n",
      "training loss: 0.0017218138508462695\n",
      "validation loss: 0.002139779080169563\n",
      "\n",
      "starting epoch 225\n",
      "training loss: 0.001724699683999043\n",
      "validation loss: 0.0022106970089194617\n",
      "\n",
      "starting epoch 226\n",
      "training loss: 0.0017247787509872999\n",
      "validation loss: 0.0022604094525056346\n",
      "\n",
      "starting epoch 227\n",
      "training loss: 0.0017338657920056946\n",
      "validation loss: 0.002141616216566234\n",
      "\n",
      "starting epoch 228\n",
      "training loss: 0.0017174266934766565\n",
      "validation loss: 0.002192285251356571\n",
      "\n",
      "starting epoch 229\n",
      "training loss: 0.0017274217050554542\n",
      "validation loss: 0.0022859990951714575\n",
      "\n",
      "starting epoch 230\n",
      "training loss: 0.0017223813343201584\n",
      "validation loss: 0.0021960522403551876\n",
      "\n",
      "starting epoch 231\n",
      "training loss: 0.0017125682691471898\n",
      "validation loss: 0.0022665389044495762\n",
      "\n",
      "starting epoch 232\n",
      "training loss: 0.0017165640069775347\n",
      "validation loss: 0.0022164011138226577\n",
      "\n",
      "starting epoch 233\n",
      "training loss: 0.0017136122999900688\n",
      "validation loss: 0.002260190103026676\n",
      "\n",
      "starting epoch 234\n",
      "training loss: 0.0017262885397971636\n",
      "validation loss: 0.0022586593161683433\n",
      "\n",
      "starting epoch 235\n",
      "training loss: 0.0017317325634234324\n",
      "validation loss: 0.002171430590460232\n",
      "\n",
      "starting epoch 236\n",
      "training loss: 0.0017058209739945222\n",
      "validation loss: 0.0022244168127045524\n",
      "\n",
      "starting epoch 237\n",
      "training loss: 0.0017218203701042918\n",
      "validation loss: 0.002258652038707622\n",
      "\n",
      "starting epoch 238\n",
      "training loss: 0.0017225863946633724\n",
      "validation loss: 0.0021526204408936194\n",
      "\n",
      "starting epoch 239\n",
      "training loss: 0.0017116979681866748\n",
      "validation loss: 0.0022363464628121204\n",
      "\n",
      "starting epoch 240\n",
      "training loss: 0.0017182855077923272\n",
      "validation loss: 0.0021519321404230717\n",
      "\n",
      "starting epoch 241\n",
      "training loss: 0.0017206535535732627\n",
      "validation loss: 0.0022330806257386377\n",
      "\n",
      "starting epoch 242\n",
      "training loss: 0.001702635048622817\n",
      "validation loss: 0.0022639206701174025\n",
      "\n",
      "starting epoch 243\n",
      "training loss: 0.0017054504267731481\n",
      "validation loss: 0.0022380773978349115\n",
      "\n",
      "starting epoch 244\n",
      "training loss: 0.0017205497715622187\n",
      "validation loss: 0.002229184556676986\n",
      "\n",
      "starting epoch 245\n",
      "training loss: 0.001704863044273412\n",
      "validation loss: 0.0021785812099113415\n",
      "\n",
      "starting epoch 246\n",
      "training loss: 0.0017109007220875167\n",
      "validation loss: 0.002194514523122613\n",
      "\n",
      "starting epoch 247\n",
      "training loss: 0.001706272213272111\n",
      "validation loss: 0.0022380161649181206\n",
      "\n",
      "starting epoch 248\n",
      "training loss: 0.0016994573060491087\n",
      "validation loss: 0.002145725587903237\n",
      "\n",
      "starting epoch 249\n",
      "training loss: 0.0017068617102338297\n",
      "validation loss: 0.00220308962744517\n",
      "\n",
      "starting epoch 250\n",
      "training loss: 0.0017222617058355216\n",
      "validation loss: 0.002231310752573741\n",
      "\n",
      "starting epoch 251\n",
      "training loss: 0.0017042573735761444\n",
      "validation loss: 0.002223479500622016\n",
      "\n",
      "starting epoch 252\n",
      "training loss: 0.001696505720972967\n",
      "validation loss: 0.0023010174846908866\n",
      "\n",
      "starting epoch 253\n",
      "training loss: 0.0016908902750358528\n",
      "validation loss: 0.0022498888069821333\n",
      "\n",
      "starting epoch 254\n",
      "training loss: 0.0016995942317021315\n",
      "validation loss: 0.0022309401058475853\n",
      "\n",
      "starting epoch 255\n",
      "training loss: 0.0017056193646411272\n",
      "validation loss: 0.002240673234774158\n",
      "\n",
      "starting epoch 256\n",
      "training loss: 0.0017177533751049842\n",
      "validation loss: 0.002252083018898238\n",
      "\n",
      "starting epoch 257\n",
      "training loss: 0.0016951627205945392\n",
      "validation loss: 0.002237531023782022\n",
      "\n",
      "starting epoch 258\n",
      "training loss: 0.0017055411008186638\n",
      "validation loss: 0.002216403106601443\n",
      "\n",
      "starting epoch 259\n",
      "training loss: 0.0016941170644338838\n",
      "validation loss: 0.0022180139321375855\n",
      "\n",
      "starting epoch 260\n",
      "training loss: 0.0016970920357493437\n",
      "validation loss: 0.002190972552437074\n",
      "\n",
      "starting epoch 261\n",
      "training loss: 0.001702317010707226\n",
      "validation loss: 0.0022881245276276116\n",
      "\n",
      "starting epoch 262\n",
      "training loss: 0.0016924944179644208\n",
      "validation loss: 0.0021904781048884404\n",
      "\n",
      "starting epoch 263\n",
      "training loss: 0.0017059242711082708\n",
      "validation loss: 0.002205469955081716\n",
      "\n",
      "starting epoch 264\n",
      "training loss: 0.0016966439177014349\n",
      "validation loss: 0.0021721152630076947\n",
      "\n",
      "starting epoch 265\n",
      "training loss: 0.0016955080701441948\n",
      "validation loss: 0.0022396410542534894\n",
      "\n",
      "starting epoch 266\n",
      "training loss: 0.0016998079587413645\n",
      "validation loss: 0.0022145502089380895\n",
      "\n",
      "starting epoch 267\n",
      "training loss: 0.0016880849882952326\n",
      "validation loss: 0.0021936249282272714\n",
      "\n",
      "starting epoch 268\n",
      "training loss: 0.0016735331265279791\n",
      "validation loss: 0.002258845686924719\n",
      "\n",
      "starting epoch 269\n",
      "training loss: 0.0016856316140804904\n",
      "validation loss: 0.0023175657075079643\n",
      "\n",
      "starting epoch 270\n",
      "training loss: 0.0016806923335641942\n",
      "validation loss: 0.002221320002624596\n",
      "\n",
      "starting epoch 271\n",
      "training loss: 0.0017022872938226925\n",
      "validation loss: 0.002268172283590207\n",
      "\n",
      "starting epoch 272\n",
      "training loss: 0.0016828858373035146\n",
      "validation loss: 0.0022295724183844946\n",
      "\n",
      "starting epoch 273\n",
      "training loss: 0.0016963181184312976\n",
      "validation loss: 0.002217530473805443\n",
      "\n",
      "starting epoch 274\n",
      "training loss: 0.0016794995174447451\n",
      "validation loss: 0.0022211851049723425\n",
      "\n",
      "starting epoch 275\n",
      "training loss: 0.0016931623843490535\n",
      "validation loss: 0.0021902812736023414\n",
      "\n",
      "starting epoch 276\n",
      "training loss: 0.0016787593118554628\n",
      "validation loss: 0.002233766751475668\n",
      "\n",
      "starting epoch 277\n",
      "training loss: 0.001694051604298275\n",
      "validation loss: 0.0021754909100084084\n",
      "\n",
      "starting epoch 278\n",
      "training loss: 0.0016804279883436124\n",
      "validation loss: 0.0022258162211767915\n",
      "\n",
      "starting epoch 279\n",
      "training loss: 0.0016769458509306124\n",
      "validation loss: 0.0022504682728385233\n",
      "\n",
      "starting epoch 280\n",
      "training loss: 0.0016777044396483751\n",
      "validation loss: 0.0022351551958116446\n",
      "\n",
      "starting epoch 281\n",
      "training loss: 0.0016748540739472875\n",
      "validation loss: 0.0021869406735143416\n",
      "\n",
      "starting epoch 282\n",
      "training loss: 0.0016533457249395006\n",
      "validation loss: 0.002262103290922331\n",
      "\n",
      "starting epoch 283\n",
      "training loss: 0.001681457161243861\n",
      "validation loss: 0.00222567973391979\n",
      "\n",
      "starting epoch 284\n",
      "training loss: 0.0016779317233267803\n",
      "validation loss: 0.0022174238231272322\n",
      "\n",
      "starting epoch 285\n",
      "training loss: 0.0016701858160445714\n",
      "validation loss: 0.002221177568438958\n",
      "\n",
      "starting epoch 286\n",
      "training loss: 0.0016850493650967374\n",
      "validation loss: 0.002223084574730909\n",
      "\n",
      "starting epoch 287\n",
      "training loss: 0.0016751381108445383\n",
      "validation loss: 0.0022044461581576763\n",
      "\n",
      "starting epoch 288\n",
      "training loss: 0.001686096862168351\n",
      "validation loss: 0.002231063333710554\n",
      "\n",
      "starting epoch 289\n",
      "training loss: 0.001665759461686063\n",
      "validation loss: 0.002197579978492105\n",
      "\n",
      "starting epoch 290\n",
      "training loss: 0.0016771752555662758\n",
      "validation loss: 0.0021817684380999645\n",
      "\n",
      "starting epoch 291\n",
      "training loss: 0.0016753676056646034\n",
      "validation loss: 0.0021687082796685553\n",
      "\n",
      "starting epoch 292\n",
      "training loss: 0.0016598933285937641\n",
      "validation loss: 0.0021762085139338597\n",
      "\n",
      "starting epoch 293\n",
      "training loss: 0.0016643796795619405\n",
      "validation loss: 0.0022190699730667376\n",
      "\n",
      "starting epoch 294\n",
      "training loss: 0.0016588606012789254\n",
      "validation loss: 0.002214000743538554\n",
      "\n",
      "starting epoch 295\n",
      "training loss: 0.0016812304176725926\n",
      "validation loss: 0.002241885753491403\n",
      "\n",
      "starting epoch 296\n",
      "training loss: 0.0016780531684900013\n",
      "validation loss: 0.0022259499504421842\n",
      "\n",
      "starting epoch 297\n",
      "training loss: 0.0016894876427824565\n",
      "validation loss: 0.002189121973313243\n",
      "\n",
      "starting epoch 298\n",
      "training loss: 0.001657157248263787\n",
      "validation loss: 0.0022114229755687\n",
      "\n",
      "starting epoch 299\n",
      "training loss: 0.0016673098756959702\n",
      "validation loss: 0.0022557967606536837\n",
      "CPU times: user 1d 5h 35min 25s, sys: 4min 24s, total: 1d 5h 39min 50s\n",
      "Wall time: 2h 21min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Define optimizer\n",
    "#model = UNet()\n",
    "model.to('cuda') # run on gpu\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.05) # usual learning rate 0.0002\n",
    "loss_fn   = nn.L1Loss()\n",
    "epochs = 105\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nstarting epoch {epoch+195}')\n",
    "    epoch_loss=[]\n",
    "    val_temp_loss = []\n",
    "    \n",
    "    #if epoch == 10:\n",
    "        #optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001) # reduce loss as given epoch\n",
    "    \n",
    "    #loop through training data \n",
    "    for (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = torch.clamp(model(igram.to('cuda'), dem.to('cuda')), -1, 1) # Generate noise predictions\n",
    "    \n",
    "        # calculate predicted signals \n",
    "        signal_pred = igram.to('cuda')-out.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(signal_pred.to('cuda'), signal_target.to('cuda')) # calculate loss \n",
    "        epoch_loss.append(loss.item()) # add batch loss to epoch loss list\n",
    "        \n",
    "        loss.backward() #Propagate the gradients in backward pass\n",
    "        optimizer.step() \n",
    "\n",
    "    train_loss.append(np.mean(epoch_loss))\n",
    "    print(f'training loss: {np.mean(epoch_loss)}')\n",
    "    \n",
    "    # run model on validation data \n",
    "    for (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in val_loader:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = torch.clamp(model(igram.to('cuda'), dem.to('cuda')), -1, 1) #Generate predictions using the model\n",
    "            \n",
    "            signal_pred = igram.to('cuda')-out.to('cuda')\n",
    "           \n",
    "            loss = loss_fn(signal_pred.to('cuda'), signal_target.to('cuda')) #Loss/error\n",
    "            val_temp_loss.append(loss.item())\n",
    "    \n",
    "    val_loss.append(np.mean(val_temp_loss))\n",
    "    print(f'validation loss: {np.mean(val_temp_loss)}')\n",
    "    \n",
    "    if (epoch+1)%5 == 0: \n",
    "        # save model\n",
    "        torch.save(model.state_dict(), f'../models/noisemodel1.3_{epoch+1+195}epochs')\n",
    "        \n",
    "    with open('../loss/val_loss_p2.pkl', 'wb') as f:\n",
    "        pickle.dump(val_loss, f)\n",
    "        \n",
    "    with open('../loss/train_loss_p2.pkl', 'wb') as f:\n",
    "        pickle.dump(train_loss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeefdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24058131-65e5-4e07-82c8-0c6e5db3d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../loss/val_loss.pkl', 'wb') as f:\n",
    "#     pickle.load(val_loss, f)\n",
    "        \n",
    "# with open('../loss/train_loss.pkl', 'wb') as f:\n",
    "#     pickle.load(train_loss, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss over all epochs\n",
    "f, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(train_loss, label='training')\n",
    "ax.plot(val_loss, label='validation')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('L1 loss')\n",
    "ax.set_title('Loss')\n",
    "ax.legend()\n",
    "plt.savefig('loss.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ef8a2-9b31-44cc-8da2-5fe8fa6b9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw inputs and outputs\n",
    "num_images=5\n",
    "\n",
    "for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "        with torch.no_grad():\n",
    "            noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "            signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "            \n",
    "            f, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "            ax[0][0].imshow(igram.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None) \n",
    "            ax[0][0].set_title('original interferogram')\n",
    "            ax[0][1].imshow(noise_target.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "            ax[0][1].set_title('true noise')\n",
    "            ax[0][2].imshow(noise.squeeze().to('cpu'), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "            ax[0][2].set_title('predicted noise')\n",
    "            ax[1][0].imshow(dem.squeeze().to('cpu'), cmap='viridis', vmin=-1, vmax=1, interpolation=None)\n",
    "            ax[1][0].set_title('DEM')\n",
    "            ax[1][1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-0.05, vmax=0.05, interpolation=None)\n",
    "            ax[1][1].set_title('true signal')\n",
    "            ax[1][2].imshow(signal.squeeze(), cmap='RdBu_r', vmin=-0.05, vmax=0.05, interpolation=None)\n",
    "            ax[1][2].set_title('predicted signal')\n",
    "  \n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(f'pred_raw{i}.png', dpi=300)\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ce385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-normalized inputs and outputs\n",
    "num_images=5\n",
    "for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "        with torch.no_grad():\n",
    "            noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "            signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "            \n",
    "            igram = undo_norm(igram.squeeze())\n",
    "            signal = undo_norm(signal.squeeze())\n",
    "            noise = undo_norm(noise.squeeze())\n",
    "            signal_target = undo_norm(signal_target.squeeze())\n",
    "            noise_target = undo_norm(noise_target.squeeze())\n",
    "            \n",
    "            f, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "            ax[0][0].imshow(igram, cmap='RdBu_r', vmin=-10, vmax=10, interpolation=None) \n",
    "            ax[0][0].set_title('original interferogram')\n",
    "            ax[0][1].imshow(noise_target.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10, interpolation=None)\n",
    "            ax[0][1].set_title('true noise')\n",
    "            ax[0][2].imshow(noise.squeeze().to('cpu'), cmap='RdBu_r', vmin=-10, vmax=10, interpolation=None)\n",
    "            ax[0][2].set_title('predicted noise')\n",
    "            ax[1][0].imshow(dem.squeeze().to('cpu'), cmap='viridis', vmin=-1, vmax=1, interpolation=None)\n",
    "            ax[1][0].set_title('DEM')\n",
    "            ax[1][1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1, interpolation=None)\n",
    "            ax[1][1].set_title('true signal')\n",
    "            ax[1][2].imshow(signal.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1, interpolation=None)\n",
    "            ax[1][2].set_title('predicted signal')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(f'pred_raw{i}.png', dpi=300)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705142f",
   "metadata": {},
   "source": [
    "## Evaluate results with training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4bf65-0f37-4987-b08b-703b9420a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloaders for evaluation\n",
    "val_data_ssim = dataset(val_list, val_d, transform=my_transforms, blurnoise=True)\n",
    "train_data_ssim = dataset(train_list, train_d, transform=my_transforms, blurnoise=True)\n",
    "val_loader_ssim = torch.utils.data.DataLoader(dataset = val_data_ssim, batch_size=1, shuffle=True)\n",
    "train_loader_ssim = torch.utils.data.DataLoader(dataset = train_data_ssim, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091c20f-d5c0-406d-ab8a-e5e1bbd0c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_lists(model, data_loader, data_range=82):\n",
    "    # initialize lists \n",
    "    ssim_list_uncorrected = []\n",
    "    ssim_list_model = []\n",
    "    ssim_list_era5 = []\n",
    "    ssim_list_murp = []\n",
    "    ssim_list_hp = []\n",
    "    \n",
    "    for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(data_loader):\n",
    "        # model preds\n",
    "        noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "        signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "        \n",
    "        # denormalize\n",
    "        igram = undo_norm(igram.squeeze().detach())\n",
    "        signal = undo_norm(signal.squeeze().detach())\n",
    "        signal_target = undo_norm(signal_target.squeeze().detach())\n",
    "        \n",
    "        # uncorrected SSIM\n",
    "        # calc ssim\n",
    "        ssim_value_uncorrected = ssim(igram.numpy(), signal_target.numpy(), gaussian_weights=True, data_range=data_range)\n",
    "        ssim_list_uncorrected.append(ssim_value_uncorrected)\n",
    "        \n",
    "        \n",
    "        # calc ssim\n",
    "        ssim_value_model = ssim(signal.numpy(), signal_target.numpy(), gaussian_weights=True, data_range=data_range)\n",
    "        ssim_list_model.append(ssim_value_model)\n",
    "    \n",
    "        # era5 corrected SSIM\n",
    "        ssim_value_era5 = ssim(era5_corr.squeeze().numpy(), signal_target.numpy(),gaussian_weights=True, data_range=data_range)\n",
    "        ssim_list_era5.append(ssim_value_era5)\n",
    "\n",
    "        # murp corrected SSIM\n",
    "        ssim_value_murp = ssim(murp_corr.squeeze().numpy(), signal_target.numpy(),gaussian_weights=True, data_range=data_range)\n",
    "        ssim_list_murp.append(ssim_value_murp)\n",
    "    \n",
    "        # hp filter corrected SSIM\n",
    "        ssim_value_hp = ssim(hp_corr.squeeze().numpy(), signal_target.numpy(), gaussian_weights=True, data_range=data_range)\n",
    "        ssim_list_hp.append(ssim_value_hp)\n",
    "    \n",
    "    print('mean ssim before correction:', np.mean(ssim_list_uncorrected),\n",
    "          '\\nmean ssim model correction:', np.mean(ssim_list_model), \n",
    "          '\\nmean ssim era5 correction:', np.mean(ssim_list_era5),\n",
    "          '\\nmean ssim murp correction:', np.mean(ssim_list_murp),\n",
    "          '\\nmean ssim high pass filter correction:', np.mean(ssim_list_hp))\n",
    "    \n",
    "    return ssim_list_uncorrected, ssim_list_model, ssim_list_era5, ssim_list_murp, ssim_list_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672befb5-4113-48b1-bc20-4c95900c3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_lists(model, data_loader):\n",
    "    # initialize lists \n",
    "    mse_list_uncorrected = []\n",
    "    mse_list_model = []\n",
    "    mse_list_era5 = []\n",
    "    mse_list_murp = []\n",
    "    mse_list_hp = []\n",
    "    \n",
    "    for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(data_loader):\n",
    "        \n",
    "        # model preds\n",
    "        noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "        signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1).detach()\n",
    "        \n",
    "        # denormalize\n",
    "        igram = undo_norm(igram.squeeze().detach())\n",
    "        signal = undo_norm(signal.squeeze().detach())\n",
    "        signal_target = undo_norm(signal_target.squeeze())\n",
    "        \n",
    "        # uncorrected MSE\n",
    "        # calc ssim\n",
    "        mse_value_uncorrected = mse(igram.numpy(), signal_target.numpy())\n",
    "        mse_list_uncorrected.append(mse_value_uncorrected)\n",
    "    \n",
    "        # Model corrected MSE\n",
    "        # calc ssim\n",
    "        mse_value_model = mse(signal.numpy(), signal_target.numpy())\n",
    "        mse_list_model.append(mse_value_model)\n",
    "    \n",
    "        # era5 corrected MSE\n",
    "        mse_value_era5 = mse(era5_corr.squeeze().numpy(), signal_target.numpy())\n",
    "        mse_list_era5.append(mse_value_era5)\n",
    "\n",
    "        # murp corrected MSE\n",
    "        mse_value_murp = mse(murp_corr.squeeze().numpy(), signal_target.numpy())\n",
    "        mse_list_murp.append(mse_value_murp)\n",
    "    \n",
    "        # hp filter corrected MSE\n",
    "        mse_value_hp = mse(hp_corr.squeeze().numpy(), signal_target.numpy())\n",
    "        mse_list_hp.append(mse_value_hp)\n",
    "    \n",
    "    print('median ssim before correction:', np.mean(mse_list_uncorrected),\n",
    "          '\\nmean mse model correction:', np.mean(mse_list_model), \n",
    "          '\\nmean mse era5 correction:', np.mean(mse_list_era5),\n",
    "          '\\nmean mse murp correction:', np.mean(mse_list_murp),\n",
    "          '\\nmean mse high pass filter correction:', np.mean(mse_list_hp))\n",
    "    \n",
    "    return mse_list_uncorrected, mse_list_model, mse_list_era5, mse_list_murp, mse_list_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f103dd-b7c7-42b6-9f6e-3e9e3641220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('val data ssim')\n",
    "val_ssim_list_uncorrected, val_ssim_list_model, val_ssim_list_era5, val_ssim_list_murp, val_ssim_list_hp = ssim_lists(model, val_loader_ssim)\n",
    "print('training data ssim')\n",
    "train_ssim_list_uncorrected, train_ssim_list_model, train_ssim_list_era5, train_ssim_list_murp, train_ssim_list_hp = ssim_lists(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff7801-06d4-4bfe-af59-4f853acd4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('val data mse')\n",
    "val_mse_list_uncorrected, val_mse_list_model, val_mse_list_era5, val_mse_list_murp, val_mse_list_hp = mse_lists(model, val_loader_ssim)\n",
    "print('training data mse')\n",
    "train_mse_list_uncorrected, train_mse_list_model, train_mse_list_era5, train_mse_list_murp, train_mse_list_hp = mse_lists(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf8616-f708-4468-a87f-b1be211f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SNR\n",
    "def rms(tensor):\n",
    "    rms = np.sqrt(np.mean(tensor.squeeze().numpy()**2))\n",
    "    return rms\n",
    "\n",
    "def snr(model, data_loader):\n",
    "    snr_list = []\n",
    "\n",
    "    for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(data_loader):\n",
    "        igram = undo_norm(igram.squeeze().detach())\n",
    "        signal_target = undo_norm(signal_target.squeeze().detach())\n",
    "        snr_list.append(rms(signal_target)/rms(igram-signal_target))\n",
    "\n",
    "    print('mean snr of images:', np.mean(snr_list), 'stdev of SNR of images:', np.std(snr_list))\n",
    "    \n",
    "    return snr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785ce4f-4b81-46c5-9449-1a5016d4bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_snr_list = snr(model, val_loader_ssim)\n",
    "train_snr_list = snr(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9ab51-2289-47a6-bebd-42b4048ad98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(tensor):\n",
    "    rms = np.sqrt(np.mean(tensor.squeeze().numpy()**2))\n",
    "    return rms\n",
    "\n",
    "def snr_single(signal_target, igram):\n",
    "    snr_val = (rms(signal_target)/rms(igram-signal_target))\n",
    "    return snr_val\n",
    "\n",
    "def ssim_single(signal_target, pred, data_range=81):\n",
    "    ssim_val = ssim(pred.squeeze().detach().numpy(), \n",
    "                    signal_target.squeeze().detach().numpy(), gaussian_weights=True, data_range=data_range)\n",
    "    return ssim_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf500a-6fd3-41d4-88f0-dd107c99ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example corrections for paper fig\n",
    "plt.style.use('default')\n",
    "\n",
    "num_images = 1\n",
    "\n",
    "for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader_ssim):\n",
    "    if i < num_images:\n",
    "        noise_pred = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "        signal_pred = torch.clamp(igram.to('cpu') - noise_pred.to('cpu'), -1, 1)\n",
    "\n",
    "        igram = undo_norm(igram.squeeze().detach())\n",
    "        signal_target = undo_norm(signal_target.squeeze())\n",
    "        noise_target = undo_norm(noise_target.squeeze())\n",
    "        noise_pred = undo_norm(noise_pred.squeeze().detach().to('cpu'))\n",
    "        signal_pred = undo_norm(signal_pred.squeeze().detach().to('cpu'))\n",
    "        \n",
    "        # Interferogram SNR\n",
    "        print(f'interferogram SNR: {snr_single(signal_target, igram)}')\n",
    "        # Uncorrected ssim\n",
    "        print(f'uncorrected SSIM: {ssim_single(signal_target, igram)}')\n",
    "        # CNN corrected ssim\n",
    "        print(f'CNN corrected SSIM: {ssim_single(signal_target, signal_pred)}')\n",
    "        # ERA5 corrected ssim\n",
    "        print(f'ERA5 corrected SSIM: {ssim_single(signal_target, era5_corr)}')\n",
    "        # murp corrected ssim\n",
    "        print(f'MuRP corrected SSIM: {ssim_single(signal_target, murp_corr)}')\n",
    "        # HP corrected ssim\n",
    "        print(f'HP corrected SSIM: {ssim_single(signal_target, hp_corr)}')\n",
    "        \n",
    "        f, ax = plt.subplots(2, 5, figsize=(12,5))\n",
    "        # interferogram\n",
    "        ax[0, 0].imshow(igram, cmap='RdBu_r', vmin=-10, vmax=10) \n",
    "        ax[0, 0].set_title('training')\n",
    "        ax[0, 0].axis('off')\n",
    "        # dem\n",
    "        ax[0, 1].imshow(dem.squeeze(), cmap='viridis') \n",
    "        ax[0, 1].set_title('DEM')\n",
    "        ax[0, 1].axis('off')\n",
    "        # target signal\n",
    "        ax[0, 2].imshow(signal_target, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[0, 2].set_title('target signal')\n",
    "        ax[0, 2].axis('off')\n",
    "        #target noise\n",
    "        ax[0, 3].imshow(noise_target, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[0, 3].set_title('target noise')\n",
    "        ax[0, 3].axis('off')\n",
    "        # CNN noise prediction\n",
    "        ax[1, 0].imshow(noise_pred, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 0].set_title('model noise prediction')\n",
    "        ax[1, 0].axis('off')\n",
    "        # CNN signal prediction\n",
    "        ax[1, 1].imshow(signal_pred, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 1].set_title('model corrected')\n",
    "        ax[1, 1].axis('off')\n",
    "        # ERA5 signal prediction\n",
    "        ax[1, 2].imshow(era5_corr.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 2].set_title('ERA5 corrected')\n",
    "        ax[1, 2].axis('off')\n",
    "        # murp signal prediction\n",
    "        ax[1, 3].imshow(murp_corr.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 3].set_title('MuRP corrected')\n",
    "        ax[1, 3].axis('off')\n",
    "        # HP signal prediction\n",
    "        ax[1, 4].imshow(hp_corr.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 4].set_title('hp filter corrected')\n",
    "        ax[1, 4].axis('off')\n",
    "        f.tight_layout()\n",
    "        \n",
    "        #plt.savefig('pred_example.png', dpi=300)\n",
    "    \n",
    "        if i+1 >= num_images:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165639a-70b1-4efe-9d85-738df7c394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_for_plotting(snr_list, ssim_list_uncorrected, ssim_list_model, ssim_list_era5, ssim_list_murp, ssim_list_hp):\n",
    "\n",
    "    roll_count = 200\n",
    "    q_low = 25\n",
    "    q_high = 75\n",
    "\n",
    "    ssim_dict = {'snr': snr_list,\n",
    "                 'ssim_uncorrected':ssim_list_uncorrected,\n",
    "                 'ssim_model':ssim_list_model,\n",
    "                 'ssim_era5':ssim_list_era5,\n",
    "                 'ssim_murp':ssim_list_murp,\n",
    "                 'ssim_hp':ssim_list_hp}\n",
    "    ssim_df = pd.DataFrame(ssim_dict)\n",
    "\n",
    "    # uncorrected ssim\n",
    "    ssim_df['ssim_uncorrected_mean'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_uncorrected_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_uncorrected_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # model corrected ssim\n",
    "    ssim_df['ssim_model_mean'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_model_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_model_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # era5 corrected ssim\n",
    "    ssim_df['ssim_era5_mean'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_era5_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_era5_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # murp corrected ssim\n",
    "    ssim_df['ssim_murp_mean'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_murp_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_murp_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # era5 corrected ssim\n",
    "    ssim_df['ssim_hp_mean'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_hp_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_hp_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "    \n",
    "    return ssim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6d18c-cbc9-4bb3-838d-1069f780603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ssim_df=df_for_plotting(val_snr_list, val_ssim_list_uncorrected, val_ssim_list_model, val_ssim_list_era5, val_ssim_list_murp, val_ssim_list_hp)\n",
    "train_ssim_df=df_for_plotting(train_snr_list, train_ssim_list_uncorrected, train_ssim_list_model, train_ssim_list_era5, train_ssim_list_murp, train_ssim_list_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b338a4-598d-4693-9a11-879291ece522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't plot exactly 0 snr due to log scale\n",
    "train_ssim_df_clean = train_ssim_df[train_ssim_df.snr != 0]\n",
    "val_ssim_df_clean = val_ssim_df[val_ssim_df.snr != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c04f4-566f-4bfe-bbdd-fd1610fe4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "# val uncorrected \n",
    "sns.histplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False, alpha=0)\n",
    "ax[0, 0].set_xscale('log')\n",
    "\n",
    "sns.histplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 0].set_xscale('log')\n",
    "ax[0, 0].set_ylabel('SSIM')\n",
    "ax[0, 0].set_title('uncorrected')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val model corrected\n",
    "sns.histplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 1].set_xscale('log')\n",
    "ax[0, 1].set_ylabel('SSIM')\n",
    "ax[0, 1].set_title('CNN')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val era5 corrected\n",
    "sns.histplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 2].set_xscale('log')\n",
    "ax[0, 2].set_ylabel('SSIM')\n",
    "ax[0, 2].set_title('ERA5')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val murp corrected\n",
    "sns.histplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 3].set_xscale('log')\n",
    "ax[0, 3].set_ylabel('SSIM')\n",
    "ax[0, 3].set_title('MuRP')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "# val hp corrected\n",
    "sns.histplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 4].set_xscale('log')\n",
    "ax[0, 4].set_xlabel('SNR')\n",
    "ax[0, 4].set_ylabel('SSIM')\n",
    "ax[0, 4].set_title('low-pass filter')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train uncorrected \n",
    "sns.histplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 0].set_xscale('log')\n",
    "ax[1, 0].set_ylabel('SSIM')\n",
    "ax[1, 0].set_xlabel('SNR')\n",
    "\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train model corrected\n",
    "sns.histplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 1].set_xscale('log')\n",
    "ax[1, 1].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train era5 corrected\n",
    "sns.histplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 2].set_xscale('log')\n",
    "ax[1, 2].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train murp corrected\n",
    "sns.histplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 3].set_xscale('log')\n",
    "ax[1, 3].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train hp corrected\n",
    "sns.histplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 4].set_xscale('log')\n",
    "ax[1, 4].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "#plt.savefig('SSIMv2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719a80d-7da1-4dd9-bb9a-6271e766cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mse_df=df_for_plotting(val_snr_list, val_mse_list_uncorrected, val_mse_list_model, val_mse_list_era5, val_mse_list_murp, val_mse_list_hp)\n",
    "train_mse_df=df_for_plotting(train_snr_list, train_mse_list_uncorrected, train_mse_list_model, train_mse_list_era5, train_mse_list_murp, train_mse_list_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3374c8d-3683-4856-ab83-4d0dc8c168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't plot exactly 0 snr due to log scale\n",
    "train_mse_df_clean = train_mse_df[train_ssim_df.snr != 0]\n",
    "val_mse_df_clean = val_mse_df[val_ssim_df.snr != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d43554-b150-4e06-a31d-804314d05e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "# val uncorrected \n",
    "sns.histplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False, alpha=0)\n",
    "ax[0, 0].set_xscale('log')\n",
    "ax[0, 0].set_yscale('log')\n",
    "\n",
    "sns.histplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 0].set_xscale('log')\n",
    "ax[0, 0].set_yscale('log')\n",
    "ax[0, 0].set_ylabel('MSE')\n",
    "ax[0, 0].set_title('uncorrected')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val model corrected\n",
    "sns.histplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 1].set_xscale('log')\n",
    "ax[0, 1].set_yscale('log')\n",
    "ax[0, 1].set_ylabel('MSE')\n",
    "ax[0, 1].set_title('CNN')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val era5 corrected\n",
    "sns.histplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 2].set_xscale('log')\n",
    "ax[0, 2].set_yscale('log')\n",
    "ax[0, 2].set_ylabel('MSE')\n",
    "ax[0, 2].set_title('ERA5')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val murp corrected\n",
    "sns.histplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 3].set_xscale('log')\n",
    "ax[0, 3].set_yscale('log')\n",
    "ax[0, 3].set_ylabel('MSE')\n",
    "ax[0, 3].set_title('MuRP')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "# val hp corrected\n",
    "sns.histplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 4].set_xscale('log')\n",
    "ax[0, 4].set_yscale('log')\n",
    "ax[0, 4].set_xlabel('SNR')\n",
    "ax[0, 4].set_ylabel('MSE')\n",
    "ax[0, 4].set_title('low-pass filter')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train uncorrected \n",
    "sns.histplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 0].set_xscale('log')\n",
    "ax[1, 0].set_yscale('log')\n",
    "ax[1, 0].set_ylabel('MSE')\n",
    "ax[1, 0].set_xlabel('SNR')\n",
    "\n",
    "sns.lineplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train model corrected\n",
    "sns.histplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 1].set_xscale('log')\n",
    "ax[1, 1].set_yscale('log')\n",
    "ax[1, 1].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train era5 corrected\n",
    "sns.histplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 2].set_xscale('log')\n",
    "ax[1, 2].set_yscale('log')\n",
    "ax[1, 2].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train murp corrected\n",
    "sns.histplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 3].set_xscale('log')\n",
    "ax[1, 3].set_yscale('log')\n",
    "ax[1, 3].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train hp corrected\n",
    "sns.histplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 4].set_xscale('log')\n",
    "ax[1, 4].set_yscale('log')\n",
    "ax[1, 4].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "#plt.savefig('SSIMv2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab2cb0-6400-435c-8f1a-ab3248e432be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to long format for more plotting\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "val_ssim_long = pd.melt(val_ssim_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "val_ssim_long['dataset'] = 'val'\n",
    "\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "train_ssim_long = pd.melt(train_ssim_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "train_ssim_long['dataset'] = 'train'\n",
    "\n",
    "all_ssim_long = pd.concat([train_ssim_long, val_ssim_long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f25b0-98fb-4d9f-a84c-c167e3e36f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to long format for more plotting\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "val_mse_long = pd.melt(val_mse_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "val_mse_long['dataset'] = 'val'\n",
    "\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "train_mse_long = pd.melt(train_mse_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "train_mse_long['dataset'] = 'train'\n",
    "\n",
    "all_mse_long = pd.concat([train_mse_long, val_mse_long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b6e15-e001-4101-9aee-2a2f6d569f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplots \n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "sns.kdeplot(ax=ax[1], data=val_ssim_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "sns.kdeplot(ax=ax[0], data=train_ssim_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "ax[1].set_xlim((-0.25, 1.15))\n",
    "ax[0].set_xlim((-0.25, 1.15))\n",
    "ax[1].set_xlabel('SSIM')\n",
    "ax[0].set_xlabel('SSIM')\n",
    "ax[1].set_ylabel('kernel density')\n",
    "ax[0].set_ylabel('kernel density')\n",
    "f.tight_layout()\n",
    "#plt.savefig('ssim_kde.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e8db9-3ba8-48ea-abf8-56e7decdf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplots \n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "sns.kdeplot(ax=ax[1], data=val_mse_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "sns.kdeplot(ax=ax[0], data=train_mse_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "ax[1].set_xlim((-0.25, 4))\n",
    "ax[0].set_xlim((-0.25, 4))\n",
    "ax[1].set_xlabel('MSE')\n",
    "ax[0].set_xlabel('MSE')\n",
    "ax[1].set_ylabel('kernel density')\n",
    "ax[0].set_ylabel('kernel density')\n",
    "f.tight_layout()\n",
    "#plt.savefig('ssim_kde.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0ba7c-7f84-43d6-9afd-1a7e63b074f0",
   "metadata": {},
   "source": [
    "## testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80781210-740f-4d8c-9fed-2f58d9bbd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define dataloader for evaluation\n",
    "# test_data = dataset(test_list, test_d, transform=my_transforms, blurnoise=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_data_ssim, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258b42f-fa3b-4f56-9b63-d62252f2bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('test data ssim')\n",
    "# test_ssim_list_uncorrected, test_ssim_list_model, test_ssim_list_era5, test_ssim_list_murp, test_ssim_list_hp = ssim_lists(model, test_loader)\n",
    "# print('test data mse')\n",
    "# test_mse_list_uncorrected, test_mse_list_model, test_mse_list_era5, test_mse_list_murp, test_mse_list_hp = mse_lists(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a47b2-1a4f-4c42-acbf-75565129d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_snr_list = snr(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e0143-8ec9-4fc3-8b7f-c5e5ac65b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all values for plot\n",
    "# def list_all(data_loader):\n",
    "#     # init lists\n",
    "#     uncorrected_list = []\n",
    "#     target_list = []\n",
    "#     model_list = []\n",
    "#     era5_list = []\n",
    "#     murp_list = []\n",
    "#     hp_list = []\n",
    "\n",
    "#     for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(data_loader):\n",
    "#         if (i+1)%100 == 0:\n",
    "#             print(f'loop {i+1}/{data.filelength}')\n",
    "\n",
    "#         noise_pred = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "#         signal_pred = torch.clamp(igram.to('cpu') - noise_pred.to('cpu'), -1, 1)\n",
    "\n",
    "#         igram = undo_norm(igram.squeeze().detach())\n",
    "#         signal_target = undo_norm(signal_target.squeeze())\n",
    "#         signal_pred = undo_norm(signal_pred.squeeze().detach().to('cpu'))\n",
    "\n",
    "#         uncorrected_list.extend(igram.flatten())\n",
    "#         target_list.extend(signal_target.flatten())\n",
    "#         model_list.extend(signal_pred.flatten())\n",
    "#         era5_list.extend(era5_corr.flatten())\n",
    "#         murp_list.extend(murp_corr.flatten())\n",
    "#         hp_list.extend(hp_corr.flatten())\n",
    "    \n",
    "#     return uncorrected_list, target_list, model_list, era5_list, murp_list, hp_list\n",
    "\n",
    "# uncorrected_list, target_list, model_list, era5_list, murp_list, hp_list = list_all(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b14d7-75fa-42a2-9763-f1ade10fc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rmse plot\n",
    "# def plot_test_rmse(uncorrected_list, target_list, model_list, era5_list, murp_list, hp_list,\n",
    "#                    binwidth=5, vmax=5000, axmin=-50, axmax=50)\n",
    "\n",
    "#     sns.set_theme()\n",
    "#     f, ax = plt.subplots(1, 5, figsize=(10,4), sharex=True, sharey=True)\n",
    "\n",
    "#     # uncorrected \n",
    "#     sns.histplot(ax=ax[0], x=target_list, y=uncorrected_list, \n",
    "#                  cmap='Greens', cbar=False, alpha=1, binwidth=binwidth, vmax=vmax)\n",
    "#     ax[0].set_xlabel('target signal')\n",
    "#     ax[0].set_ylabel('uncorrected signal')\n",
    "#     ax[0].set_title('Before correction')\n",
    "#     ax[0].set_box_aspect(1)\n",
    "#     ax[0].set_xlim(axmin, axmax)\n",
    "#     ax[0].set_ylim(axmin, axmax)\n",
    "#     #ax[0].set_yticks([-0.5, 0, 0.5, 1])\n",
    "\n",
    "#     # cnn predictions\n",
    "#     sns.histplot(ax=ax[1], x=target_list, y=model_list, \n",
    "#                  cmap='Greens', cbar=False, alpha=1, binwidth=binwidth, vmax=vmax)\n",
    "#     ax[1].set_xlabel('target signal')\n",
    "#     ax[1].set_ylabel('predicted signal')\n",
    "#     ax[1].set_title('CNN predicton')\n",
    "#     ax[1].set_box_aspect(1)\n",
    "\n",
    "#     # era5 predictions\n",
    "#     sns.histplot(ax=ax[2], x=target_list, y=era5_list, \n",
    "#                  cmap='Greens', cbar=False, alpha=1, binwidth=binwidth, vmax=vmax)\n",
    "#     ax[2].set_xlabel('target signal')\n",
    "#     ax[2].set_title('ERA5 prediction')\n",
    "#     ax[2].set_box_aspect(1)\n",
    "\n",
    "#     # murp predictions\n",
    "#     sns.histplot(ax=ax[3], x=target_list, y=murp_list, \n",
    "#                  cmap='Greens', cbar=False, alpha=1, binwidth=binwidth, vmax=vmax)\n",
    "#     ax[3].set_xlabel('target signal')\n",
    "#     ax[3].set_title('MuRP prediction')\n",
    "#     ax[3].set_box_aspect(1)\n",
    "\n",
    "#     sns.histplot(ax=ax[4], x=target_list, y=hp_list, \n",
    "#                  cmap='Greens', cbar=False, alpha=1, binwidth=binwidth, vmax=vmax)\n",
    "#     ax[4].set_xlabel('target signal')\n",
    "#     ax[4].set_title('high-pass filter prediction')\n",
    "#     ax[4].set_box_aspect(1)\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     #plt.savefig('pred_example_resid.png', dpi=300)\n",
    "    \n",
    "# plot_test_rmse(uncorrected_list, target_list, model_list, era5_list, murp_list, hp_list,\n",
    "#                binwidth=5, vmax=5000, axmin=-50, axmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc4c36-19fa-4ded-8e68-6b8f13ac2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ssim_df=df_for_plotting(test_snr_list, test_ssim_list_uncorrected, test_ssim_list_model, test_ssim_list_era5, test_ssim_list_murp, test_ssim_list_hp)\n",
    "# test_mse_df=df_for_plotting(test_snr_list, test_mse_list_uncorrected, test_mse_list_model, test_mse_list_era5, test_mse_list_murp, test_mse_list_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39a96b-c537-4e13-bf9b-9ed8539c2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can't plot exactly 0 snr due to log scale\n",
    "# test_ssim_df_clean = test_ssim_df[test_ssim_df.snr != 0]\n",
    "# test_mse_df_clean = test_mse_df[test_mse_df.snr != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60f31b-addb-418d-9e0b-1763e35c5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_theme()\n",
    "\n",
    "# f, ax = plt.subplots(1, 5, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "# # val uncorrected \n",
    "# sns.histplot(ax=ax[0], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False, alpha=0)\n",
    "# ax[0].set_xscale('log')\n",
    "\n",
    "# sns.histplot(ax=ax[0], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[0].set_xscale('log')\n",
    "# ax[0].set_ylabel('SSIM')\n",
    "# ax[0].set_title('uncorrected')\n",
    "\n",
    "# sns.lineplot(ax=ax[0], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[0], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[0], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# # val model corrected\n",
    "# sns.histplot(ax=ax[1], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_model, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[1].set_xscale('log')\n",
    "# ax[1].set_ylabel('SSIM')\n",
    "# ax[1].set_title('CNN')\n",
    "\n",
    "# sns.lineplot(ax=ax[1], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[1], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[1], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# # val era5 corrected\n",
    "# sns.histplot(ax=ax[2], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_era5, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[2].set_xscale('log')\n",
    "# ax[2].set_ylabel('SSIM')\n",
    "# ax[2].set_title('ERA5')\n",
    "\n",
    "# sns.lineplot(ax=ax[2], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[2], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[2], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# # val murp corrected\n",
    "# sns.histplot(ax=ax[3], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_murp, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[3].set_xscale('log')\n",
    "# ax[3].set_ylabel('SSIM')\n",
    "# ax[3].set_title('MuRP')\n",
    "\n",
    "# sns.lineplot(ax=ax[3], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[3], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[3], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "# # val hp corrected\n",
    "# sns.histplot(ax=ax[4], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_hp, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[4].set_xscale('log')\n",
    "# ax[4].set_xlabel('SNR')\n",
    "# ax[4].set_ylabel('SSIM')\n",
    "# ax[4].set_title('low-pass filter')\n",
    "\n",
    "# sns.lineplot(ax=ax[4], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[4], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[4], x=test_ssim_df_clean.snr, y=test_ssim_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# #plt.savefig('SSIMv2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf52372-209b-4376-98ed-52b594133ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_theme()\n",
    "\n",
    "# f, ax = plt.subplots(1, 5, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "# # val uncorrected \n",
    "# sns.histplot(ax=ax[0], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False, alpha=0)\n",
    "# ax[0].set_xscale('log')\n",
    "\n",
    "# sns.histplot(ax=ax[0], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[0].set_xscale('log')\n",
    "# ax[0].set_ylabel('MSE')\n",
    "# ax[0].set_title('uncorrected')\n",
    "\n",
    "# sns.lineplot(ax=ax[0], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[0], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[0], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# # val model corrected\n",
    "# sns.histplot(ax=ax[1], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_model, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[1].set_xscale('log')\n",
    "# ax[1].set_ylabel('MSE')\n",
    "# ax[1].set_title('CNN')\n",
    "\n",
    "# sns.lineplot(ax=ax[1], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[1], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[1], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# # val era5 corrected\n",
    "# sns.histplot(ax=ax[2], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_era5, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[2].set_xscale('log')\n",
    "# ax[2].set_ylabel('MSE')\n",
    "# ax[2].set_title('ERA5')\n",
    "\n",
    "# sns.lineplot(ax=ax[2], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[2], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[2], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# # val murp corrected\n",
    "# sns.histplot(ax=ax[3], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_murp, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[3].set_xscale('log')\n",
    "# ax[3].set_ylabel('MSE')\n",
    "# ax[3].set_title('MuRP')\n",
    "\n",
    "# sns.lineplot(ax=ax[3], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[3], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[3], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "# # val hp corrected\n",
    "# sns.histplot(ax=ax[4], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_hp, cmap='Oranges', cbar=False,\n",
    "#              bins=30, vmax=50, alpha=0.8)\n",
    "# ax[4].set_xscale('log')\n",
    "# ax[4].set_xlabel('SNR')\n",
    "# ax[4].set_ylabel('MSE')\n",
    "# ax[4].set_title('low-pass filter')\n",
    "\n",
    "# sns.lineplot(ax=ax[4], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[4], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "# sns.lineplot(ax=ax[4], x=test_mse_df_clean.snr, y=test_mse_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# #plt.savefig('SSIMv2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b78de0-7fb9-4719-b88d-479c321d8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to long format for more plotting\n",
    "# ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "# test_ssim_long = pd.melt(test_ssim_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "# test_ssim_long['dataset'] = 'test'\n",
    "\n",
    "# # convert to long format for more plotting\n",
    "# ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "# test_mse_long = pd.melt(test_mse_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='mse')\n",
    "# test_mse_long['dataset'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ec585-a0aa-4ea4-8e30-900f7025ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #histplots \n",
    "# sns.set_theme()\n",
    "# f, ax = plt.subplots(1, 1, figsize=(8,3))\n",
    "\n",
    "# sns.kdeplot(ax=ax, data=test_ssim_long, x='ssim', hue='corr_type', \n",
    "#             hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "#             palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "# ax.set_xlim((-0.25, 1.15))\n",
    "# ax.set_xlabel('SSIM')\n",
    "# ax.set_ylabel('kernel density')\n",
    "# f.tight_layout()\n",
    "# #plt.savefig('ssim_kde.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b77d5-2149-425b-ad1e-2d8842a54015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #histplots \n",
    "# sns.set_theme()\n",
    "# f, ax = plt.subplots(1, 1, figsize=(8,3))\n",
    "\n",
    "# sns.kdeplot(ax=ax, data=test_mse_long, x='mse', hue='corr_type', \n",
    "#             hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "#             palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "# #ax.set_xlim((-0.25, 1.15))\n",
    "# ax.set_xlabel('MSE')\n",
    "# ax.set_ylabel('kernel density')\n",
    "# f.tight_layout()\n",
    "# #plt.savefig('ssim_kde.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
