{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fc1947",
   "metadata": {},
   "source": [
    "# InSAR denoiser training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6286b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import torch\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "import random\n",
    "from scipy import stats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879cfea",
   "metadata": {},
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b238030-bcca-46ec-abfd-fda11c63fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/mnt/d/indennt'\n",
    "subset_types = ['signal', 'noise', 'dem', 'murp', 'era5']\n",
    "\n",
    "# exclude non tif files, e.g. metadata\n",
    "def list_tifs(my_fns):\n",
    "    my_list = []\n",
    "    for i in my_fns:\n",
    "        if i[-4:] == '.tif':\n",
    "            my_list.append(i)\n",
    "    return my_list\n",
    "\n",
    "def subset_lists(main_dir, ds_type, subset_types):\n",
    "    path_d = {}\n",
    "    fn_list = []\n",
    "    for type in subset_types:\n",
    "        path_d[type] = f'{main_dir}/{ds_type}_subsets/{type}/'\n",
    "        fn_list.append(list_tifs(os.listdir(path_d[type])))\n",
    "    return path_d, fn_list[0]\n",
    "\n",
    "train_d, train_list = subset_lists(main_dir, 'train', subset_types)\n",
    "val_d, val_list = subset_lists(main_dir, 'val', subset_types)\n",
    "test_d, test_list = subset_lists(main_dir, 'test', subset_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f43d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c39167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset \n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, path_d, norm_list = [-41, 41, 0, 4374.6372], transform=None, \n",
    "                 norm=True, blurnoise=False, random_blur=False, flip=False, invert=False, rotate=False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.signal_dir = path_d['signal']\n",
    "        self.noise_dir = path_d['noise']\n",
    "        self.dem_dir = path_d['dem']\n",
    "        self.era5_dir = path_d['era5']\n",
    "        self.murp_dir = path_d['murp']\n",
    "        self.norm = norm\n",
    "        self.blurnoise = blurnoise\n",
    "        self.random_blur = random_blur\n",
    "        self.flip = flip\n",
    "        self.invert = invert\n",
    "        self.rotate = rotate\n",
    "        self.igram_min = norm_list[0]\n",
    "        self.igram_max = norm_list[1]\n",
    "        self.dem_min = norm_list[2]\n",
    "        self.dem_max = norm_list[3]\n",
    "        \n",
    "    #dataset length\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "    \n",
    "    #load images\n",
    "    def __getitem__(self,idx):\n",
    "        signal_path = self.signal_dir+self.file_list[idx]\n",
    "        noise_path = self.noise_dir+self.file_list[idx]\n",
    "        dem_path = self.dem_dir+self.file_list[idx]\n",
    "        era5_path = self.era5_dir+self.file_list[idx]\n",
    "        murp_path = self.murp_dir+self.file_list[idx]\n",
    "        \n",
    "        signal = self.transform(Image.open(signal_path))\n",
    "        noise = self.transform(Image.open(noise_path))\n",
    "        dem = self.transform(Image.open(dem_path))\n",
    "        era5 = self.transform(Image.open(era5_path))\n",
    "        murp = self.transform(Image.open(murp_path))\n",
    "        \n",
    "        # Blur noise\n",
    "        if self.blurnoise == True: # blur noise to mitigate signal from non atmospheric sources\n",
    "            if self.random_blur == True:\n",
    "                blur_params = [[13, 3], [15, 3.5], [17, 4], [19, 4.5], [21, 5]]\n",
    "                while True:\n",
    "                    index = round(np.random.normal(2, 0.4))\n",
    "                    if -1 < index < 5:\n",
    "                        break\n",
    "                \n",
    "                blur_select = blur_params[index]\n",
    "                gblur = transforms.GaussianBlur(kernel_size=(blur_select[0], blur_select[0]), sigma=blur_select[1])\n",
    "                noise = gblur(noise)\n",
    "            else:\n",
    "                gblur = transforms.GaussianBlur(kernel_size=(17, 17), sigma=4)\n",
    "                noise = gblur(noise)\n",
    "        \n",
    "        # Generate scaled training images\n",
    "        scalar = np.round(np.random.lognormal(4.5, 0.2), 3) # FOR PLOTTING: 5, 0.2, for training 4, 0.5\n",
    "        signal = signal*scalar*-1 #multiply by -1 because mintpy has a reversed sign convention\n",
    "        igram = noise+signal\n",
    "        \n",
    "        # set local ref for era5\n",
    "        ref_index = signal.abs().argmin().item() # location of lowest signal in velocity map\n",
    "        corr_diff = (igram.flatten()[ref_index] - era5.flatten()[ref_index]).item()\n",
    "        era5 = era5+corr_diff\n",
    "        \n",
    "        # set local ref for murp\n",
    "        corr_diff = (igram.flatten()[ref_index] - murp.flatten()[ref_index]).item()\n",
    "        murp = murp+corr_diff\n",
    "        \n",
    "        # correct era5 and murp\n",
    "        era5_corr = igram-era5\n",
    "        murp_corr = igram-murp\n",
    "        \n",
    "        # correct hp\n",
    "        hp_filter = transforms.GaussianBlur(kernel_size=(17, 17), sigma=4)\n",
    "        igram_filtered = hp_filter(igram)\n",
    "        hp_corr = igram - igram_filtered\n",
    "        \n",
    "        if self.invert==True:\n",
    "            if random.random() < 0.5:\n",
    "                igram = igram*-1\n",
    "                signal = signal*-1\n",
    "                noise = noise*-1\n",
    "                era5_corr = era5_corr *-1\n",
    "                murp_corr = murp_corr *-1\n",
    "                hp_corr = hp_corr *-1\n",
    "            \n",
    "        if self.norm == True:\n",
    "            igram = 2*(((igram-self.igram_min)/(self.igram_max-self.igram_min)))-1\n",
    "            signal = 2*(((signal-self.igram_min)/(self.igram_max-self.igram_min)))-1\n",
    "            noise = 2*(((noise-self.igram_min)/(self.igram_max-self.igram_min)))-1\n",
    "            dem = 2*(((dem-self.dem_min)/(self.dem_max-self.dem_min)))-1\n",
    "        \n",
    "        if self.flip==True:\n",
    "            flip_dim = []\n",
    "            if random.random() < 0.25:\n",
    "                flip_dim = [0]\n",
    "            elif random.random() > 0.25 and random.random() < 0.5:\n",
    "                flip_dim = [1]\n",
    "            elif random.random() > 0.5 and random.random() < 0.75:\n",
    "                flip_dim = [0, 1]\n",
    "            \n",
    "            igram = torch.flip(igram, flip_dim)\n",
    "            signal = torch.flip(signal, flip_dim)\n",
    "            noise = torch.flip(noise, flip_dim)\n",
    "            era5_corr = torch.flip(era5_corr, flip_dim)\n",
    "            murp_corr = torch.flip(murp_corr, flip_dim)\n",
    "            hp_corr = torch.flip(hp_corr, flip_dim)\n",
    "            dem = torch.flip(dem, flip_dim)\n",
    "        \n",
    "        if self.rotate==True:\n",
    "            angle = random.randint(0, 180)\n",
    "            igram = transforms.functional.rotate(igram, angle)\n",
    "            signal = transforms.functional.rotate(signal, angle)\n",
    "            noise = transforms.functional.rotate(noise, angle)\n",
    "            era5_corr = transforms.functional.rotate(era5_corr, angle)\n",
    "            murp_corr = transforms.functional.rotate(murp_corr, angle)\n",
    "            hp_corr = transforms.functional.rotate(hp_corr, angle)\n",
    "            dem = transforms.functional.rotate(dem, angle)\n",
    "            \n",
    "         \n",
    "        return igram, signal, noise, dem, era5_corr, murp_corr, hp_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049ef07-e52a-48dd-ae1f-8a268a3502e0",
   "metadata": {},
   "source": [
    "### Find dataset max and min for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14433430-133e-4506-aea6-69a5d14279c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset min and max for normalization\n",
    "# def get_max(data_list, data_d, transforms=my_transforms):\n",
    "#     max = 0 \n",
    "#     min = 0\n",
    "#     data = dataset(data_list, data_d, transform=transforms, blurnoise=True, norm=False)\n",
    "#     loader = torch.utils.data.DataLoader(dataset = data, batch_size=1, shuffle=False)\n",
    "#     for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(loader):\n",
    "#         if (i+1)%1000 == 0: \n",
    "#             print(f'loop {i+1}/{data.filelength}')\n",
    "#         if igram.max() > max:\n",
    "#             max = igram.max()\n",
    "\n",
    "#     return max\n",
    "\n",
    "# get_max(train_list, train_d)\n",
    "\n",
    "# # train igram max = 35.4796\n",
    "# # val igram max = 27.1600\n",
    "# # will set at 37 to be safe\n",
    "\n",
    "# # train dem max, min: 4374.6372, 0\n",
    "# # val dem max, min: 4353.3779, 0\n",
    "\n",
    "# # updating snr for run 8: \n",
    "# # train igram max: 36.0537\n",
    "# # will set at 37 to be safe\n",
    "\n",
    "# # updating snr for run 9 \n",
    "# # train igram max: 39.7634\n",
    "# # will set at 41 to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80e875e-3e03-403e-844e-eec379a1d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_norm(array, min=-41, max=41):\n",
    "    array = ((array+1)*((max-min)/2))+min\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48154e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data = dataset(train_list, train_d, transform=my_transforms, blurnoise=True, random_blur=True, flip=True, invert=True, rotate=True)\n",
    "val_data = dataset(val_list, val_d, transform=my_transforms, blurnoise=True, random_blur=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9287c",
   "metadata": {},
   "source": [
    "## Examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5ba1c-dddd-47bb-96e5-fc441c9b047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot all original data \n",
    "# num_images = 5\n",
    "\n",
    "# for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "#     if i < num_images:\n",
    "#         igram = undo_norm(igram.squeeze())\n",
    "#         signal_target = undo_norm(signal_target.squeeze())\n",
    "#         noise_target = undo_norm(noise_target.squeeze())\n",
    "        \n",
    "#         f, ax = plt.subplots(2, 5, figsize=(10,5), sharey=True, sharex=True)\n",
    "#         ax[0][0].imshow(igram, cmap='RdBu_r', vmin=-10, vmax=10) \n",
    "#         ax[0][0].set_title('training')\n",
    "#         ax[0][1].imshow(signal_target, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "#         ax[0][1].set_title('target signal')\n",
    "#         ax[0][2].imshow(era5_corr.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "#         ax[0][2].set_title('ERA5 corrected')\n",
    "#         ax[0][3].imshow(murp_corr.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "#         ax[0][3].set_title('MuRP corrected')\n",
    "#         ax[0][4].imshow(hp_corr.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "#         ax[0][4].set_title('HP corrected')\n",
    "        \n",
    "#         ax[1][0].imshow(dem.squeeze(), cmap='viridis') \n",
    "#         ax[1][0].set_title('DEM')\n",
    "#         ax[1][1].imshow(noise_target, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "#         ax[1][1].set_title('target noise')\n",
    "#         ax[1][2].imshow((igram-era5_corr.squeeze()), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "#         ax[1][2].set_title('ERA5 noise')\n",
    "#         ax[1][3].imshow((igram-murp_corr.squeeze()), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "#         ax[1][3].set_title('MuRP noise')\n",
    "#         ax[1][4].imshow((igram-hp_corr.squeeze()), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "#         ax[1][4].set_title('HP noise')\n",
    "#         f.tight_layout()\n",
    "#     else:\n",
    "#         break\n",
    "            \n",
    "#             #plt.savefig(f'input_correctons{i}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot normalized training inputs \n",
    "# num_images = 5\n",
    "\n",
    "# for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "#     if i < num_images:\n",
    "#             f, ax = plt.subplots(1, 4, figsize=(10,4), sharey=True)\n",
    "#             ax[0].imshow(igram.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None) \n",
    "#             ax[0].set_title('training')\n",
    "#             ax[1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "#             ax[1].set_title('target signal')\n",
    "#             ax[2].imshow(noise_target.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "#             ax[2].set_title('target noise')\n",
    "#             ax[3].imshow(dem.squeeze(), cmap='viridis', vmin=-1, vmax=1, interpolation=None)\n",
    "#             ax[3].set_title('DEM')\n",
    "#             f.tight_layout()\n",
    "#     else: \n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21150611",
   "metadata": {},
   "source": [
    "## Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd5508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "def check_valid_activation(choice):\n",
    "    if choice not in ['relu', 'lrelu', 'prelu']:\n",
    "        raise ValueError(f\"'{choice}' is not a valid activation function. Choose among ['relu', 'lrelu', 'prelu'].\\n\")\n",
    "\n",
    "\n",
    "def upconv(in_channels, out_channels, mode='transpose'):\n",
    "    # stride=2 implies upsampling by a factor of 2\n",
    "    get_up_mode = nn.ModuleDict([\n",
    "        ['bilinear', nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2), conv1x1(in_channels, out_channels))],\n",
    "        ['transpose', nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)]\n",
    "    ])\n",
    "\n",
    "    return get_up_mode[mode]\n",
    "\n",
    "\n",
    "def get_activation(choice):\n",
    "    activation_functions = nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['lrelu', nn.LeakyReLU(inplace=True)],\n",
    "        ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "    return activation_functions[choice]\n",
    "\n",
    "\n",
    "def conv_block(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Partial encoder block consisting of a 3×3 convolutional layer with stride 1, followed by batch normalization\n",
    "    (optional) and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "def conv_up_block(in_channels, out_channels, activation='relu', do_BN=True, up_mode='transpose', *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Decoder block consisting of an up-convolutional layer, followed by a 3×3 convolutional layer with stride 1,\n",
    "    batch normalization (optional), and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "\n",
    "\n",
    "def bottleneck(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Bottleneck block.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipConnection, self).__init__()\n",
    "\n",
    "    def forward(self, x_skip, x_up):\n",
    "        return x_skip + x_up\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_input_channels=2, start_kernel=64, max_filter_depth=512, depth=5,\n",
    "                 act_fn_encoder='relu', act_fn_decoder='relu', act_fn_bottleneck='relu', up_mode='transpose',\n",
    "                 do_BN=False, bias_conv_layer=True, outer_skip=True, outer_skip_BN=False):\n",
    "        \"\"\"\n",
    "        UNet network architecture.\n",
    "        :param n_input_channels:    int, number of input channels\n",
    "        :param start_kernel:        int, number of filters of the first convolutional layer in the encoder\n",
    "        :param max_filter_depth:    int, maximum filter depth\n",
    "        :param depth:               int, number of downsampling and upsampling layers (i.e., number of blocks in the\n",
    "                                    encoder and decoder)\n",
    "        :param act_fn_encoder:      str, activation function used in the encoder\n",
    "        :param act_fn_decoder:      str, activation function used in the decoder\n",
    "        :param act_fn_bottleneck:   str, activation function used in the bottleneck\n",
    "        :param up_mode:             str, upsampling mode\n",
    "        :param do_BN:               boolean, True to perform batch normalization after every convolutional layer,\n",
    "                                    False otherwise\n",
    "        :param bias_conv_layer:     boolean, True to activate the learnable bias of the convolutional layers,\n",
    "                                    False otherwise\n",
    "        :param outer_skip:          boolean, True to activate the long residual skip connection that adds the\n",
    "                                    initial DSM to the output of the last decoder layer, False otherwise\n",
    "        :param outer_skip_BN:       boolean, True to add batch normalization to the long residual skip connection,\n",
    "                                    False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        check_valid_activation(act_fn_encoder)\n",
    "        check_valid_activation(act_fn_decoder)\n",
    "        check_valid_activation(act_fn_bottleneck)\n",
    "\n",
    "        if up_mode not in ['transpose', 'bilinear']:\n",
    "            raise ValueError(f\"'{up_mode}' is not a valid mode for upsampling. Choose among ['transpose', 'bilinear'] \"\n",
    "                             \"to specify 'up_mode'.\\n\")\n",
    "\n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.start_kernel = start_kernel\n",
    "        self.depth = depth\n",
    "        self.act_fn_encoder = act_fn_encoder\n",
    "        self.act_fn_decoder = act_fn_decoder\n",
    "        self.act_fn_bottleneck = act_fn_bottleneck\n",
    "        self.up_mode = up_mode\n",
    "        self.max_filter_depth = max_filter_depth\n",
    "        self.do_BN = do_BN\n",
    "        self.bias_conv_layer = bias_conv_layer\n",
    "        self.do_outer_skip = outer_skip\n",
    "        self.do_outer_skip_BN = outer_skip_BN\n",
    "        self.filter_depths = [self.start_kernel * (2 ** i) for i in range(self.depth)]\n",
    "\n",
    "        # Restrict the maximum filter depth to a predefined value\n",
    "        self.filter_depths = [self.max_filter_depth if i > self.max_filter_depth else i for i in self.filter_depths]\n",
    "\n",
    "        # Set up the encoder\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.encoder.append(nn.Sequential(\n",
    "            conv_block(self.n_input_channels, self.start_kernel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths, self.filter_depths[1:]):\n",
    "            self.encoder.append(nn.Sequential(\n",
    "                conv_block(in_channel, out_channel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        # Set up the bottleneck\n",
    "        self.bottleneck = bottleneck(self.filter_depths[-1], self.filter_depths[-1], activation=self.act_fn_bottleneck,\n",
    "                                     do_BN=self.do_BN)\n",
    "\n",
    "        # Set up the decoder\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.filter_depths_up = list(reversed(self.filter_depths))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths_up[:-1], self.filter_depths_up[1:]):\n",
    "            self.decoder.append(conv_up_block(in_channel, out_channel, activation=self.act_fn_decoder,\n",
    "                                              up_mode=self.up_mode, do_BN=self.do_BN))\n",
    "        self.decoder.append(upconv(self.filter_depths_up[-1], self.filter_depths_up[-1], up_mode))\n",
    "\n",
    "        # Set up the final layer of the decoder\n",
    "        self.last_layer = conv3x3(self.start_kernel, 1, bias=self.bias_conv_layer)\n",
    "\n",
    "        # Skip connection\n",
    "        self.skipconnect = SkipConnection()\n",
    "\n",
    "        # Batch normalization added to the long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            self.layer_outer_skip = nn.ModuleList()\n",
    "            if self.do_outer_skip_BN:\n",
    "                self.layer_outer_skip.append(nn.BatchNorm2d(1))\n",
    "            self.layer_outer_skip.append(SkipConnection())\n",
    "\n",
    "    def forward(self, x, dem):\n",
    "        skip_connections = []\n",
    "        x = torch.cat((x, dem), dim=1)\n",
    "        out = x\n",
    "\n",
    "        # Encoder (save intermediate outputs for skip connections)\n",
    "        for index, layer in enumerate(self.encoder):\n",
    "            layer_conv = layer[:-1]  # all layers before the pooling layer (at depth index)\n",
    "            layer_pool = layer[-1]   # pooling layer (at depth index)\n",
    "\n",
    "            out_before_pool = layer_conv(out)\n",
    "            skip_connections.append(out_before_pool)\n",
    "            out = layer_pool(out_before_pool)\n",
    "\n",
    "        # Bottleneck\n",
    "        out = self.bottleneck(out)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        index_max = len(self.decoder) - 1\n",
    "        for index, layer in enumerate(self.decoder):\n",
    "            if index <= index_max - 1:\n",
    "                layer_upconv = layer[0]  # upconv layer\n",
    "                layer_conv = layer[1::]  # all other layers (conv, batchnorm, activation)\n",
    "\n",
    "                out_temp = layer_upconv(out)\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "                out = layer_conv(out)\n",
    "            else:\n",
    "                out_temp = layer(out)   # upconv of last layer\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "\n",
    "        # Last layer of the decoder\n",
    "        out = self.last_layer(out)\n",
    "\n",
    "        # Add long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            if self.layer_outer_skip.__len__() == 2:\n",
    "                # pipe input through a batch normalization layer before adding it to the output of the last\n",
    "                # decoder layer\n",
    "                bn = self.layer_outer_skip[0]\n",
    "                x_0 = x[:, 0, :, :]       # use channel 0 only\n",
    "                x_0 = x_0.unsqueeze(1)\n",
    "                x = bn(x_0)\n",
    "\n",
    "            # add (batchnorm) input to the output of the last decoder layer\n",
    "            add = self.layer_outer_skip[-1]\n",
    "            x_0 = x[:, 0, :, :]\n",
    "            x_0 = x_0.unsqueeze(1)\n",
    "\n",
    "            out = add(x_0, out)  # use channel 0 only\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df2ba7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc04b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (last_layer): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (skipconnect): SkipConnection()\n",
       "  (layer_outer_skip): ModuleList(\n",
       "    (0): SkipConnection()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load previous model\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load('../../models/noisemodel1.4_174epochs'))\n",
    "model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9060e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# #Define optimizer\n",
    "# model = UNet()\n",
    "# model.to('cuda') # run on gpu\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.1)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "# loss_fn   = nn.L1Loss()\n",
    "# epochs = 300\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# train_loss = []\n",
    "# val_loss = []\n",
    "# counter = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f'\\nstarting epoch {epoch}')\n",
    "#     epoch_loss = []\n",
    "#     val_temp_loss = []\n",
    "    \n",
    "#     #loop through training data \n",
    "#     for (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in train_loader:\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         out = torch.clamp(model(igram.to('cuda'), dem.to('cuda')), -1, 1) # Generate noise predictions\n",
    "    \n",
    "#         # calculate predicted signals \n",
    "#         signal_pred = igram.to('cuda')-out.to('cuda')\n",
    "#         loss = loss_fn(signal_pred.to('cuda'), signal_target.to('cuda')) # calculate loss \n",
    "#         epoch_loss.append(loss.item()) # add batch loss to epoch loss list\n",
    "        \n",
    "#         loss.backward() #Propagate the gradients in backward pass\n",
    "#         optimizer.step() \n",
    "\n",
    "#     train_loss.append(np.mean(epoch_loss))\n",
    "#     print(f'training loss: {np.mean(epoch_loss)}')\n",
    "    \n",
    "#     scheduler.step(np.mean(epoch_loss))\n",
    "    \n",
    "#     # run model on validation data \n",
    "#     for (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in val_loader:\n",
    "#         with torch.no_grad():\n",
    "#             model.eval()\n",
    "            \n",
    "#             out = torch.clamp(model(igram.to('cuda'), dem.to('cuda')), -1, 1) #Generate predictions using the model\n",
    "#             signal_pred = igram.to('cuda')-out.to('cuda')\n",
    "           \n",
    "#             loss = loss_fn(signal_pred.to('cuda'), signal_target.to('cuda')) #Loss/error\n",
    "#             val_temp_loss.append(loss.item())\n",
    "    \n",
    "#     # calculate loss over previous 10 epochs for early stopping later\n",
    "#     if epoch > 20:\n",
    "#         past_loss = np.mean(val_loss[-20:-10])\n",
    "    \n",
    "#     val_loss.append(np.mean(val_temp_loss))\n",
    "#     print(f'validation loss: {np.mean(val_temp_loss)}')\n",
    "        \n",
    "#     with open('../../loss/val_loss.pkl', 'wb') as f:\n",
    "#         pickle.dump(val_loss, f)\n",
    "        \n",
    "#     with open('../../loss/train_loss.pkl', 'wb') as f:\n",
    "#         pickle.dump(train_loss, f)\n",
    "    \n",
    "    \n",
    "#     # implement early stopping\n",
    "#     if epoch > 20:\n",
    "#         current_loss = np.mean(val_loss[-10:-1])\n",
    "#         if current_loss > past_loss:\n",
    "#             counter +=1\n",
    "#             if counter >= 10:\n",
    "#                 torch.save(model.state_dict(), f'../../models/noisemodel1.4_{epoch+1}epochs')\n",
    "#                 print('early stopping triggered')\n",
    "#                 break\n",
    "#         else:\n",
    "#             counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeefdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24058131-65e5-4e07-82c8-0c6e5db3d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../loss/val_loss.pkl', 'rb') as f:\n",
    "#     val_loss = pickle.load(f)\n",
    "        \n",
    "# with open('../loss/train_loss.pkl', 'rb') as f:\n",
    "#     train_loss = pickle.load(f)\n",
    "\n",
    "# with open('../loss/val_loss_p2.pkl', 'rb') as f:\n",
    "#     val_loss_p2 = pickle.load(f)\n",
    "        \n",
    "# with open('../loss/train_loss_p2.pkl', 'rb') as f:\n",
    "#     train_loss_p2 = pickle.load(f)\n",
    "\n",
    "# train_loss.extend(train_loss_p2)\n",
    "# val_loss.extend(val_loss_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot loss over all epochs\n",
    "# f, ax = plt.subplots(figsize=(10,5))\n",
    "# ax.plot(train_loss, label='training')\n",
    "# ax.plot(val_loss, label='validation')\n",
    "# ax.set_xlabel('epoch')\n",
    "# ax.set_ylabel('L1 loss')\n",
    "# ax.set_title('Loss')\n",
    "# ax.legend()\n",
    "# plt.savefig('../../figs/loss_1.4.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ef8a2-9b31-44cc-8da2-5fe8fa6b9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # raw inputs and outputs\n",
    "# num_images=5\n",
    "\n",
    "# for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "#     if i < num_images:\n",
    "#         with torch.no_grad():\n",
    "#             noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "#             signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "            \n",
    "#             f, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "#             ax[0][0].imshow(igram.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None) \n",
    "#             ax[0][0].set_title('original interferogram')\n",
    "#             ax[0][1].imshow(noise_target.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "#             ax[0][1].set_title('true noise')\n",
    "#             ax[0][2].imshow(noise.squeeze().to('cpu'), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "#             ax[0][2].set_title('predicted noise')\n",
    "#             ax[1][0].imshow(dem.squeeze().to('cpu'), cmap='viridis', vmin=-1, vmax=1, interpolation=None)\n",
    "#             ax[1][0].set_title('DEM')\n",
    "#             ax[1][1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "#             ax[1][1].set_title('true signal')\n",
    "#             ax[1][2].imshow(signal.squeeze(), cmap='RdBu_r', vmin=-0.25, vmax=0.25, interpolation=None)\n",
    "#             ax[1][2].set_title('predicted signal')\n",
    "  \n",
    "#             plt.tight_layout()\n",
    "#             #plt.savefig(f'pred_raw{i}.png', dpi=300)\n",
    "#     else: \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ce385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # un-normalized inputs and outputs\n",
    "# num_images=5\n",
    "# for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader):\n",
    "#     if i < num_images:\n",
    "#         with torch.no_grad():\n",
    "#             noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "#             signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "            \n",
    "#             igram = undo_norm(igram.squeeze())\n",
    "#             signal = undo_norm(signal.squeeze())\n",
    "#             noise = undo_norm(noise.squeeze())\n",
    "#             signal_target = undo_norm(signal_target.squeeze())\n",
    "#             noise_target = undo_norm(noise_target.squeeze())\n",
    "            \n",
    "#             f, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "#             ax[0][0].imshow(igram, cmap='RdBu_r', vmin=-10, vmax=10, interpolation=None) \n",
    "#             ax[0][0].set_title('original interferogram')\n",
    "#             ax[0][1].imshow(noise_target.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10, interpolation=None)\n",
    "#             ax[0][1].set_title('true noise')\n",
    "#             ax[0][2].imshow(noise.squeeze().to('cpu'), cmap='RdBu_r', vmin=-10, vmax=10, interpolation=None)\n",
    "#             ax[0][2].set_title('predicted noise')\n",
    "#             ax[1][0].imshow(dem.squeeze().to('cpu'), cmap='viridis', vmin=-1, vmax=1, interpolation=None)\n",
    "#             ax[1][0].set_title('DEM')\n",
    "#             ax[1][1].imshow(signal_target.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1, interpolation=None)\n",
    "#             ax[1][1].set_title('true signal')\n",
    "#             ax[1][2].imshow(signal.squeeze(), cmap='RdBu_r', vmin=-1, vmax=1, interpolation=None)\n",
    "#             ax[1][2].set_title('predicted signal')\n",
    "            \n",
    "#             plt.tight_layout()\n",
    "#             #plt.savefig(f'pred_raw{i}.png', dpi=300)\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705142f",
   "metadata": {},
   "source": [
    "## Evaluate results with training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b4bf65-0f37-4987-b08b-703b9420a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloaders for evaluation\n",
    "val_data_ssim = dataset(val_list, val_d, transform=my_transforms, blurnoise=True, random_blur=True)\n",
    "train_data_ssim = dataset(train_list, train_d, transform=my_transforms, blurnoise=True, random_blur=True)\n",
    "val_loader_ssim = torch.utils.data.DataLoader(dataset = val_data_ssim, batch_size=1, shuffle=True)\n",
    "train_loader_ssim = torch.utils.data.DataLoader(dataset = train_data_ssim, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a091c20f-d5c0-406d-ab8a-e5e1bbd0c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_lists(model, data_loader):\n",
    "    # initialize lists \n",
    "    ssim_list_uncorrected = []\n",
    "    ssim_list_model = []\n",
    "    ssim_list_era5 = []\n",
    "    ssim_list_murp = []\n",
    "    ssim_list_hp = []\n",
    "    \n",
    "    for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(data_loader):\n",
    "        # model preds\n",
    "        noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "        signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1)\n",
    "        \n",
    "        # denormalize\n",
    "        igram = undo_norm(igram.squeeze().detach())\n",
    "        signal = undo_norm(signal.squeeze().detach())\n",
    "        signal_target = undo_norm(signal_target.squeeze().detach())\n",
    "        \n",
    "        # uncorrected SSIM\n",
    "        # calc ssim\n",
    "        ssim_value_uncorrected = ssim(signal_target.numpy(), igram.numpy(), gaussian_weights=True, data_range=(igram.max()-igram.min()).item())\n",
    "        ssim_list_uncorrected.append(ssim_value_uncorrected)\n",
    "        \n",
    "        \n",
    "        # calc ssim\n",
    "        ssim_value_model = ssim(signal_target.numpy(), signal.numpy(), gaussian_weights=True,  data_range=(signal.max()-signal.min()).item())\n",
    "        ssim_list_model.append(ssim_value_model)\n",
    "    \n",
    "        # era5 corrected SSIM\n",
    "        ssim_value_era5 = ssim(signal_target.numpy(), era5_corr.squeeze().numpy(), gaussian_weights=True, data_range=(era5_corr.max()-era5_corr.min()).item())\n",
    "        ssim_list_era5.append(ssim_value_era5)\n",
    "\n",
    "        # murp corrected SSIM\n",
    "        ssim_value_murp = ssim(signal_target.numpy(), murp_corr.squeeze().numpy(), gaussian_weights=True, data_range=(murp_corr.max()-murp_corr.min()).item())\n",
    "        ssim_list_murp.append(ssim_value_murp)\n",
    "    \n",
    "        # hp filter corrected SSIM\n",
    "        ssim_value_hp = ssim(signal_target.numpy(), hp_corr.squeeze().numpy(), gaussian_weights=True, data_range=(hp_corr.max()-hp_corr.min()).item())\n",
    "        ssim_list_hp.append(ssim_value_hp)\n",
    "    \n",
    "    print('mean ssim before correction:', np.mean(ssim_list_uncorrected),\n",
    "          '\\nmean ssim model correction:', np.mean(ssim_list_model), \n",
    "          '\\nmean ssim era5 correction:', np.mean(ssim_list_era5),\n",
    "          '\\nmean ssim murp correction:', np.mean(ssim_list_murp),\n",
    "          '\\nmean ssim high pass filter correction:', np.mean(ssim_list_hp))\n",
    "    \n",
    "    return ssim_list_uncorrected, ssim_list_model, ssim_list_era5, ssim_list_murp, ssim_list_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672befb5-4113-48b1-bc20-4c95900c3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mse_lists(model, data_loader):\n",
    "#     # initialize lists \n",
    "#     mse_list_uncorrected = []\n",
    "#     mse_list_model = []\n",
    "#     mse_list_era5 = []\n",
    "#     mse_list_murp = []\n",
    "#     mse_list_hp = []\n",
    "    \n",
    "#     for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(data_loader):\n",
    "        \n",
    "#         # model preds\n",
    "#         noise = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "#         signal = torch.clamp(igram.to('cpu') - noise.to('cpu'), -1, 1).detach()\n",
    "        \n",
    "#         # denormalize\n",
    "#         igram = undo_norm(igram.squeeze().detach())\n",
    "#         signal = undo_norm(signal.squeeze().detach())\n",
    "#         signal_target = undo_norm(signal_target.squeeze())\n",
    "        \n",
    "#         # uncorrected MSE\n",
    "#         # calc ssim\n",
    "#         mse_value_uncorrected = mse(signal_target.numpy(), igram.numpy())\n",
    "#         mse_list_uncorrected.append(mse_value_uncorrected)\n",
    "    \n",
    "#         # Model corrected MSE\n",
    "#         # calc ssim\n",
    "#         mse_value_model = mse(signal_target.numpy(), signal.numpy())\n",
    "#         mse_list_model.append(mse_value_model)\n",
    "    \n",
    "#         # era5 corrected MSE\n",
    "#         mse_value_era5 = mse(signal_target.numpy(), era5_corr.squeeze().numpy())\n",
    "#         mse_list_era5.append(mse_value_era5)\n",
    "\n",
    "#         # murp corrected MSE\n",
    "#         mse_value_murp = mse(signal_target.numpy(), murp_corr.squeeze().numpy())\n",
    "#         mse_list_murp.append(mse_value_murp)\n",
    "    \n",
    "#         # hp filter corrected MSE\n",
    "#         mse_value_hp = mse(signal_target.numpy(), hp_corr.squeeze().numpy())\n",
    "#         mse_list_hp.append(mse_value_hp)\n",
    "    \n",
    "#     print('mean rmse before correction:', np.mean(mse_list_uncorrected),\n",
    "#           '\\nmean rmse model correction:', np.mean(mse_list_model), \n",
    "#           '\\nmean rmse era5 correction:', np.mean(mse_list_era5),\n",
    "#           '\\nmean rmse murp correction:', np.mean(mse_list_murp),\n",
    "#           '\\nmean rmse high pass filter correction:', np.mean(mse_list_hp))\n",
    "    \n",
    "#     return mse_list_uncorrected, mse_list_model, mse_list_era5, mse_list_murp, mse_list_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f103dd-b7c7-42b6-9f6e-3e9e3641220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('val data ssim')\n",
    "# val_ssim_list_uncorrected, val_ssim_list_model, val_ssim_list_era5, val_ssim_list_murp, val_ssim_list_hp = ssim_lists(model, val_loader_ssim)\n",
    "# print('training data ssim')\n",
    "# train_ssim_list_uncorrected, train_ssim_list_model, train_ssim_list_era5, train_ssim_list_murp, train_ssim_list_hp = ssim_lists(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff7801-06d4-4bfe-af59-4f853acd4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('val data rmse')\n",
    "# val_mse_list_uncorrected, val_mse_list_model, val_mse_list_era5, val_mse_list_murp, val_mse_list_hp = mse_lists(model, val_loader_ssim)\n",
    "# print('training data rmse')\n",
    "# train_mse_list_uncorrected, train_mse_list_model, train_mse_list_era5, train_mse_list_murp, train_mse_list_hp = mse_lists(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5caf8616-f708-4468-a87f-b1be211f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SNR\n",
    "def rms(tensor):\n",
    "    rms = np.sqrt(np.mean(tensor.squeeze().numpy()**2))\n",
    "    return rms\n",
    "\n",
    "def snr(model, data_loader):\n",
    "    snr_list = []\n",
    "\n",
    "    for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(data_loader):\n",
    "        igram = undo_norm(igram.squeeze().detach())\n",
    "        signal_target = undo_norm(signal_target.squeeze().detach())\n",
    "        snr_list.append(rms(signal_target)/rms(igram-signal_target))\n",
    "\n",
    "    print('mean snr of images:', np.mean(snr_list), 'stdev of SNR of images:', np.std(snr_list))\n",
    "    \n",
    "    return snr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2785ce4f-4b81-46c5-9449-1a5016d4bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean snr of images: 0.25748128 stdev of SNR of images: 0.26198924\n",
      "mean snr of images: 0.27315876 stdev of SNR of images: 0.30874467\n"
     ]
    }
   ],
   "source": [
    "val_snr_list = snr(model, val_loader_ssim)\n",
    "train_snr_list = snr(model, train_loader_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b4cbd5-ab65-4475-93fa-c70b7516efb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1715776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(train_snr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9ab51-2289-47a6-bebd-42b4048ad98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(tensor):\n",
    "    rms = np.sqrt(np.mean(tensor.squeeze().numpy()**2))\n",
    "    return rms\n",
    "\n",
    "def snr_single(signal_target, igram):\n",
    "    snr_val = (rms(signal_target)/rms(igram-signal_target))\n",
    "    return snr_val\n",
    "\n",
    "def ssim_single(signal_target, pred):\n",
    "    ssim_val = ssim(signal_target.squeeze().detach().numpy(),\n",
    "                    pred.squeeze().detach().numpy(), \n",
    "                    gaussian_weights=True, data_range=(pred.max()-pred.min()).item())\n",
    "    return ssim_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf500a-6fd3-41d4-88f0-dd107c99ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example corrections for paper fig\n",
    "plt.style.use('default')\n",
    "\n",
    "num_images = 1\n",
    "\n",
    "for i, (igram, signal_target, noise_target, dem, era5_corr, murp_corr, hp_corr) in enumerate(val_loader_ssim):\n",
    "    if i < num_images:\n",
    "        noise_pred = model(igram.to('cuda'), dem.to('cuda')) #Generate predictions using the model\n",
    "        signal_pred = torch.clamp(igram.to('cpu') - noise_pred.to('cpu'), -1, 1)\n",
    "\n",
    "        igram = undo_norm(igram.squeeze().detach())\n",
    "        signal_target = undo_norm(signal_target.squeeze())\n",
    "        noise_target = undo_norm(noise_target.squeeze())\n",
    "        noise_pred = undo_norm(noise_pred.squeeze().detach().to('cpu'))\n",
    "        signal_pred = undo_norm(signal_pred.squeeze().detach().to('cpu'))\n",
    "        \n",
    "        # Interferogram SNR\n",
    "        print(f'interferogram SNR: {snr_single(signal_target, igram)}')\n",
    "        # Uncorrected ssim\n",
    "        print(f'uncorrected SSIM: {ssim_single(signal_target, igram)}')\n",
    "        # CNN corrected ssim\n",
    "        print(f'CNN corrected SSIM: {ssim_single(signal_target, signal_pred)}')\n",
    "        # ERA5 corrected ssim\n",
    "        print(f'ERA5 corrected SSIM: {ssim_single(signal_target, era5_corr)}')\n",
    "        # murp corrected ssim\n",
    "        print(f'MuRP corrected SSIM: {ssim_single(signal_target, murp_corr)}')\n",
    "        # HP corrected ssim\n",
    "        print(f'HP corrected SSIM: {ssim_single(signal_target, hp_corr)}')\n",
    "        \n",
    "        f, ax = plt.subplots(2, 5, figsize=(12,5))\n",
    "        # interferogram\n",
    "        ax[0, 0].imshow(igram, cmap='RdBu_r', vmin=-10, vmax=10) \n",
    "        ax[0, 0].set_title('training')\n",
    "        ax[0, 0].axis('off')\n",
    "        # dem\n",
    "        ax[0, 1].imshow(dem.squeeze(), cmap='viridis') \n",
    "        ax[0, 1].set_title('DEM')\n",
    "        ax[0, 1].axis('off')\n",
    "        # target signal\n",
    "        ax[0, 2].imshow(signal_target, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[0, 2].set_title('target signal')\n",
    "        ax[0, 2].axis('off')\n",
    "        #target noise\n",
    "        ax[0, 3].imshow(noise_target, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[0, 3].set_title('target noise')\n",
    "        ax[0, 3].axis('off')\n",
    "        \n",
    "        ax[0, 4].axis('off')\n",
    "        \n",
    "        # CNN noise prediction\n",
    "        ax[1, 0].imshow(noise_pred, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 0].set_title('model noise prediction')\n",
    "        ax[1, 0].axis('off')\n",
    "        # CNN signal prediction\n",
    "        ax[1, 1].imshow(signal_pred, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 1].set_title('model corrected')\n",
    "        ax[1, 1].axis('off')\n",
    "        # ERA5 signal prediction\n",
    "        ax[1, 2].imshow(era5_corr.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 2].set_title('ERA5 corrected')\n",
    "        ax[1, 2].axis('off')\n",
    "        # murp signal prediction\n",
    "        ax[1, 3].imshow(murp_corr.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 3].set_title('MuRP corrected')\n",
    "        ax[1, 3].axis('off')\n",
    "        # HP signal prediction\n",
    "        ax[1, 4].imshow(hp_corr.squeeze(), cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "        ax[1, 4].set_title('hp filter corrected')\n",
    "        ax[1, 4].axis('off')\n",
    "        f.tight_layout()\n",
    "        \n",
    "        plt.savefig('../../figs/pred_example4.png', dpi=400)\n",
    "    \n",
    "        if i+1 >= num_images:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89db8c-46f4-40e8-a351-d9ed7a363d31",
   "metadata": {},
   "source": [
    "pred_example\n",
    "interferogram SNR: 1.0959817171096802\n",
    "uncorrected SSIM: 0.49491308993844974\n",
    "CNN corrected SSIM: 0.6160475978850088\n",
    "ERA5 corrected SSIM: 0.4875292268918117\n",
    "MuRP corrected SSIM: 0.5507798440096329\n",
    "HP corrected SSIM: 0.3261401004592315\n",
    "\n",
    "pred_example2\n",
    "interferogram SNR: 0.07073532044887543\n",
    "uncorrected SSIM: 0.03941563347669969\n",
    "CNN corrected SSIM: 0.34337970765467624\n",
    "ERA5 corrected SSIM: 0.12658932071242346\n",
    "MuRP corrected SSIM: -0.053397286606253536\n",
    "HP corrected SSIM: 0.20243116222779953\n",
    "\n",
    "pred_example3\n",
    "interferogram SNR: 0.10542032867670059\n",
    "uncorrected SSIM: -0.06699941982181326\n",
    "CNN corrected SSIM: 0.47958389448036853\n",
    "ERA5 corrected SSIM: 0.3358634008980684\n",
    "MuRP corrected SSIM: 0.36387249065240096\n",
    "HP corrected SSIM: 0.14409060461297474\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165639a-70b1-4efe-9d85-738df7c394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_for_plotting(snr_list, ssim_list_uncorrected, ssim_list_model, ssim_list_era5, ssim_list_murp, ssim_list_hp):\n",
    "\n",
    "    roll_count = 200\n",
    "    q_low = 25\n",
    "    q_high = 75\n",
    "\n",
    "    ssim_dict = {'snr': snr_list,\n",
    "                 'ssim_uncorrected':ssim_list_uncorrected,\n",
    "                 'ssim_model':ssim_list_model,\n",
    "                 'ssim_era5':ssim_list_era5,\n",
    "                 'ssim_murp':ssim_list_murp,\n",
    "                 'ssim_hp':ssim_list_hp}\n",
    "    ssim_df = pd.DataFrame(ssim_dict)\n",
    "\n",
    "    # uncorrected ssim\n",
    "    ssim_df['ssim_uncorrected_mean'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_uncorrected_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_uncorrected_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_uncorrected.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # model corrected ssim\n",
    "    ssim_df['ssim_model_mean'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_model_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_model_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_model.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # era5 corrected ssim\n",
    "    ssim_df['ssim_era5_mean'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_era5_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_era5_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_era5.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # murp corrected ssim\n",
    "    ssim_df['ssim_murp_mean'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_murp_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_murp_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_murp.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "\n",
    "    # era5 corrected ssim\n",
    "    ssim_df['ssim_hp_mean'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).mean()\n",
    "    ssim_df[f'ssim_hp_q{q_low}'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).quantile(quantile=q_low/100)\n",
    "    ssim_df[f'ssim_hp_q{q_high}'] = ssim_df.sort_values(by=['snr']).ssim_hp.rolling(roll_count, center=True).quantile(quantile=q_high/100)\n",
    "    \n",
    "    return ssim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6d18c-cbc9-4bb3-838d-1069f780603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ssim_df=df_for_plotting(val_snr_list, val_ssim_list_uncorrected, val_ssim_list_model, val_ssim_list_era5, val_ssim_list_murp, val_ssim_list_hp)\n",
    "train_ssim_df=df_for_plotting(train_snr_list, train_ssim_list_uncorrected, train_ssim_list_model, train_ssim_list_era5, train_ssim_list_murp, train_ssim_list_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b338a4-598d-4693-9a11-879291ece522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't plot exactly 0 snr due to log scale\n",
    "train_ssim_df_clean = train_ssim_df[train_ssim_df.snr != 0]\n",
    "val_ssim_df_clean = val_ssim_df[val_ssim_df.snr != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c04f4-566f-4bfe-bbdd-fd1610fe4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "# val uncorrected \n",
    "sns.histplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False, alpha=0)\n",
    "ax[0, 0].set_xscale('log')\n",
    "\n",
    "sns.histplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 0].set_xscale('log')\n",
    "ax[0, 0].set_ylabel('SSIM')\n",
    "ax[0, 0].set_title('uncorrected')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val model corrected\n",
    "sns.histplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 1].set_xscale('log')\n",
    "ax[0, 1].set_ylabel('SSIM')\n",
    "ax[0, 1].set_title('CNN')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val era5 corrected\n",
    "sns.histplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 2].set_xscale('log')\n",
    "ax[0, 2].set_ylabel('SSIM')\n",
    "ax[0, 2].set_title('ERA5')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val murp corrected\n",
    "sns.histplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 3].set_xscale('log')\n",
    "ax[0, 3].set_ylabel('SSIM')\n",
    "ax[0, 3].set_title('MuRP')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "# val hp corrected\n",
    "sns.histplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 4].set_xscale('log')\n",
    "ax[0, 4].set_xlabel('SNR')\n",
    "ax[0, 4].set_ylabel('SSIM')\n",
    "ax[0, 4].set_title('low-pass filter')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_ssim_df_clean.snr, y=val_ssim_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train uncorrected \n",
    "sns.histplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 0].set_xscale('log')\n",
    "ax[1, 0].set_ylabel('SSIM')\n",
    "ax[1, 0].set_xlabel('SNR')\n",
    "\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train model corrected\n",
    "sns.histplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 1].set_xscale('log')\n",
    "ax[1, 1].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train era5 corrected\n",
    "sns.histplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 2].set_xscale('log')\n",
    "ax[1, 2].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train murp corrected\n",
    "sns.histplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 3].set_xscale('log')\n",
    "ax[1, 3].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train hp corrected\n",
    "sns.histplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 4].set_xscale('log')\n",
    "ax[1, 4].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_ssim_df_clean.snr, y=train_ssim_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "#plt.savefig('SSIMv2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719a80d-7da1-4dd9-bb9a-6271e766cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mse_df=df_for_plotting(val_snr_list, val_mse_list_uncorrected, val_mse_list_model, val_mse_list_era5, val_mse_list_murp, val_mse_list_hp)\n",
    "train_mse_df=df_for_plotting(train_snr_list, train_mse_list_uncorrected, train_mse_list_model, train_mse_list_era5, train_mse_list_murp, train_mse_list_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3374c8d-3683-4856-ab83-4d0dc8c168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't plot exactly 0 snr due to log scale\n",
    "train_mse_df_clean = train_mse_df[train_ssim_df.snr != 0]\n",
    "val_mse_df_clean = val_mse_df[val_ssim_df.snr != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d43554-b150-4e06-a31d-804314d05e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "# val uncorrected \n",
    "sns.histplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False, alpha=0)\n",
    "ax[0, 0].set_xscale('log')\n",
    "ax[0, 0].set_yscale('log')\n",
    "\n",
    "sns.histplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 0].set_xscale('log')\n",
    "ax[0, 0].set_yscale('log')\n",
    "ax[0, 0].set_ylabel('RMSE')\n",
    "ax[0, 0].set_title('uncorrected')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 0], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val model corrected\n",
    "sns.histplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 1].set_xscale('log')\n",
    "ax[0, 1].set_yscale('log')\n",
    "ax[0, 1].set_ylabel('RMSE')\n",
    "ax[0, 1].set_title('CNN')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 1], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val era5 corrected\n",
    "sns.histplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 2].set_xscale('log')\n",
    "ax[0, 2].set_yscale('log')\n",
    "ax[0, 2].set_ylabel('RMSE')\n",
    "ax[0, 2].set_title('ERA5')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 2], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# val murp corrected\n",
    "sns.histplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 3].set_xscale('log')\n",
    "ax[0, 3].set_yscale('log')\n",
    "ax[0, 3].set_ylabel('RMSE')\n",
    "ax[0, 3].set_title('MuRP')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 3], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "# val hp corrected\n",
    "sns.histplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp, cmap='Oranges', cbar=False,\n",
    "             bins=30, vmax=50, alpha=0.8)\n",
    "ax[0, 4].set_xscale('log')\n",
    "ax[0, 4].set_yscale('log')\n",
    "ax[0, 4].set_xlabel('SNR')\n",
    "ax[0, 4].set_ylabel('RMSE')\n",
    "ax[0, 4].set_title('low-pass filter')\n",
    "\n",
    "sns.lineplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[0, 4], x=val_mse_df_clean.snr, y=val_mse_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train uncorrected \n",
    "sns.histplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 0].set_xscale('log')\n",
    "ax[1, 0].set_yscale('log')\n",
    "ax[1, 0].set_ylabel('RMSE')\n",
    "ax[1, 0].set_xlabel('SNR')\n",
    "\n",
    "sns.lineplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 0], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_uncorrected_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train model corrected\n",
    "sns.histplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 1].set_xscale('log')\n",
    "ax[1, 1].set_yscale('log')\n",
    "ax[1, 1].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 1], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_model_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train era5 corrected\n",
    "sns.histplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 2].set_xscale('log')\n",
    "ax[1, 2].set_yscale('log')\n",
    "ax[1, 2].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 2], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_era5_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train murp corrected\n",
    "sns.histplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 3].set_xscale('log')\n",
    "ax[1, 3].set_yscale('log')\n",
    "ax[1, 3].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 3], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_murp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "# train hp corrected\n",
    "sns.histplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp, cmap='Blues', cbar=False, \n",
    "             bins=30, vmax=100, alpha=0.8)\n",
    "ax[1, 4].set_xscale('log')\n",
    "ax[1, 4].set_yscale('log')\n",
    "ax[1, 4].set_xlabel('SNR')\n",
    "f.tight_layout()\n",
    "\n",
    "sns.lineplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp_mean, size=1, c='k', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp_q25, size=1, c='gray', legend=False, alpha=0.6)\n",
    "sns.lineplot(ax=ax[1, 4], x=train_mse_df_clean.snr, y=train_mse_df_clean.ssim_hp_q75, size=1, c='gray', legend=False, alpha=0.6)\n",
    "\n",
    "\n",
    "#plt.savefig('SSIMv2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab2cb0-6400-435c-8f1a-ab3248e432be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to long format for more plotting\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "val_ssim_long = pd.melt(val_ssim_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "val_ssim_long['dataset'] = 'val'\n",
    "\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "train_ssim_long = pd.melt(train_ssim_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "train_ssim_long['dataset'] = 'train'\n",
    "\n",
    "all_ssim_long = pd.concat([train_ssim_long, val_ssim_long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f25b0-98fb-4d9f-a84c-c167e3e36f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to long format for more plotting\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "val_mse_long = pd.melt(val_mse_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "val_mse_long['dataset'] = 'val'\n",
    "\n",
    "ssim_labels = ['ssim_uncorrected',  'ssim_model', 'ssim_era5', 'ssim_murp', 'ssim_hp']\n",
    "train_mse_long = pd.melt(train_mse_df[ssim_labels], value_vars=ssim_labels, var_name='corr_type', value_name='ssim')\n",
    "train_mse_long['dataset'] = 'train'\n",
    "\n",
    "all_mse_long = pd.concat([train_mse_long, val_mse_long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b6e15-e001-4101-9aee-2a2f6d569f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplots \n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "sns.kdeplot(ax=ax[1], data=val_ssim_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "sns.kdeplot(ax=ax[0], data=train_ssim_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "ax[1].set_xlim((-0.35, 0.85))\n",
    "ax[0].set_xlim((-0.35, 0.85))\n",
    "ax[1].set_xlabel('SSIM')\n",
    "ax[0].set_xlabel('SSIM')\n",
    "ax[1].set_ylabel('kernel density')\n",
    "ax[0].set_ylabel('kernel density')\n",
    "f.tight_layout()\n",
    "#plt.savefig('ssim_kde.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e8db9-3ba8-48ea-abf8-56e7decdf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplots \n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "sns.kdeplot(ax=ax[1], data=val_mse_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "sns.kdeplot(ax=ax[0], data=train_mse_long, x='ssim', hue='corr_type', \n",
    "            hue_order=['ssim_hp', 'ssim_era5', 'ssim_murp', 'ssim_model', 'ssim_uncorrected'], \n",
    "            palette=['peru', 'orange', 'gold', 'blue', 'red'], fill=True, legend=False)\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[1].set_xscale('log')\n",
    "\n",
    "ax[1].set_xlabel('MSE')\n",
    "ax[0].set_xlabel('MSE')\n",
    "ax[1].set_ylabel('kernel density')\n",
    "ax[0].set_ylabel('kernel density')\n",
    "f.tight_layout()\n",
    "#plt.savefig('ssim_kde.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
